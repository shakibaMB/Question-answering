{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1d23b6c",
   "metadata": {
    "id": "d1d23b6c"
   },
   "source": [
    "# Assignment 2\n",
    "\n",
    "**Credits**: Andrea Galassi, Federico Ruggeri, Paolo Torroni\n",
    "\n",
    "**Keywords**: Transformers, Question Answering, CoQA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f451b",
   "metadata": {
    "id": "bd3f451b"
   },
   "source": [
    "## Deadlines\n",
    "\n",
    "* **December 11**, 2022: deadline for having assignments graded by January 11, 2023\n",
    "* **January 11**, 2023: deadline for half-point speed bonus per assignment\n",
    "* **After January 11**, 2023: assignments are still accepted, but there will be no speed bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ada8c8",
   "metadata": {
    "id": "11ada8c8"
   },
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c07553",
   "metadata": {
    "id": "47c07553"
   },
   "source": [
    "### Problem\n",
    "\n",
    "Question Answering (QA) on [CoQA](https://stanfordnlp.github.io/coqa/) dataset: a conversational QA dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4907f8d",
   "metadata": {
    "id": "b4907f8d"
   },
   "source": [
    "### Task\n",
    "\n",
    "Given a question $Q$, a text passage $P$, the task is to generate the answer $A$.<br>\n",
    "$\\rightarrow A$ can be: (i) a free-form text or (ii) unanswerable;\n",
    "\n",
    "**Note**: an question $Q$ can refer to previous dialogue turns. <br>\n",
    "$\\rightarrow$ dialogue history $H$ may be a valuable input to provide the correct answer $A$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3760b5",
   "metadata": {
    "id": "9b3760b5"
   },
   "source": [
    "### Models\n",
    "\n",
    "We are going to experiment with transformer-based models to define the following models:\n",
    "\n",
    "1.  $A = f_\\theta(Q, P)$\n",
    "\n",
    "2. $A = f_\\theta(Q, P, H)$\n",
    "\n",
    "where $f_\\theta$ is the transformer-based model we have to define with $\\theta$ parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cfee64",
   "metadata": {
    "id": "66cfee64"
   },
   "source": [
    "## The CoQA dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3e7d0",
   "metadata": {
    "id": "f6e3e7d0"
   },
   "source": [
    "For detailed information about the dataset, feel free to check the original [paper](https://arxiv.org/pdf/1808.07042.pdf).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb6c37e",
   "metadata": {
    "id": "bfb6c37e"
   },
   "source": [
    "## Rationales\n",
    "\n",
    "Each QA pair is paired with a rationale $R$: it is a text span extracted from the given text passage $P$. <br>\n",
    "$\\rightarrow$ $R$ is not a requested output, but it can be used as an additional information at training time!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa786e2",
   "metadata": {
    "id": "daa786e2"
   },
   "source": [
    "## Dataset Statistics\n",
    "\n",
    "* **127k** QA pairs.\n",
    "* **8k** conversations.\n",
    "* **7** diverse domains: Children's Stories, Literature, Mid/High School Exams, News, Wikipedia, Reddit, Science.\n",
    "* Average conversation length: **15 turns** (i.e., QA pairs).\n",
    "* Almost **half** of CoQA questions refer back to **conversational history**.\n",
    "* Only **train** and **validation** sets are available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26d68b7",
   "metadata": {
    "id": "d26d68b7"
   },
   "source": [
    "## Dataset snippet\n",
    "\n",
    "The dataset is stored in JSON format. Each dialogue is represented as follows:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"source\": \"mctest\",\n",
    "    \"id\": \"3dr23u6we5exclen4th8uq9rb42tel\",\n",
    "    \"filename\": \"mc160.test.41\",\n",
    "    \"story\": \"Once upon a time, in a barn near a farm house, there lived a little white kitten named Cotton. \n",
    "    Cotton lived high up in a nice warm place above the barn where all of the farmer's horses slept. [...]\" % <-- $P$\n",
    "    \"questions\": [\n",
    "        {\n",
    "            \"input_text\": \"What color was Cotton?\",   % <-- $Q_1$\n",
    "            \"turn_id\": 1\n",
    "        },\n",
    "        {\n",
    "            \"input_text\": \"Where did she live?\",\n",
    "            \"turn_id\": 2\n",
    "        },\n",
    "        [...]\n",
    "    ],\n",
    "    \"answers\": [\n",
    "        {\n",
    "            \"span_start\": 59,   % <-- $R_1$ start index\n",
    "            \"spand_end\": 93,    % <-- $R_1$ end index\n",
    "            \"span_text\": \"a little white kitten named Cotton\",   % <-- $R_1$\n",
    "            \"input_text\" \"white\",   % <-- $A_1$      \n",
    "            \"turn_id\": 1\n",
    "        },\n",
    "        [...]\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c7558c",
   "metadata": {
    "id": "72c7558c"
   },
   "source": [
    "### Simplifications\n",
    "\n",
    "Each dialogue also contains an additional field ```additional_answers```. For simplicity, we **ignore** this field and only consider one groundtruth answer $A$ and text rationale $R$.\n",
    "\n",
    "CoQA only contains 1.3% of unanswerable questions. For simplicity, we **ignore** those QA pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "RYPgl7nh3kWb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RYPgl7nh3kWb",
    "outputId": "ff74ed4e-aa41-41a0-e1c2-f37dbbb4b739"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
      "Collecting huggingface-hub<1.0,>=0.10.0\n",
      "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting datasets\n",
      "  Downloading datasets-2.8.0-py3-none-any.whl (452 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.9/452.9 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 KB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xxhash\n",
      "  Downloading xxhash-3.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (213 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m213.0/213.0 KB\u001b[0m \u001b[31m23.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.25.1)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.13-py2.py3-none-any.whl (140 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
      "Installing collected packages: xxhash, urllib3, multiprocess, responses, datasets\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 1.24.3\n",
      "    Uninstalling urllib3-1.24.3:\n",
      "      Successfully uninstalled urllib3-1.24.3\n",
      "Successfully installed datasets-2.8.0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.26.13 xxhash-3.2.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "urllib3"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01cdad7",
   "metadata": {
    "id": "e01cdad7"
   },
   "source": [
    "## [Task 1] Remove unaswerable QA pairs\n",
    "\n",
    "Write your own script to remove unaswerable QA pairs from both train and validation sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6643e14",
   "metadata": {
    "id": "f6643e14"
   },
   "source": [
    "## Dataset Download\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "358bac70",
   "metadata": {
    "id": "358bac70"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "\n",
    "class DownloadProgressBar(tqdm):\n",
    "    def update_to(self, b=1, bsize=1, tsize=None):\n",
    "        if tsize is not None:\n",
    "            self.total = tsize\n",
    "        self.update(b * bsize - self.n)\n",
    "        \n",
    "def download_url(url, output_path):\n",
    "    with DownloadProgressBar(unit='B', unit_scale=True,\n",
    "                             miniters=1, desc=url.split('/')[-1]) as t:\n",
    "        urllib.request.urlretrieve(url, filename=output_path, reporthook=t.update_to)\n",
    "\n",
    "def download_data(data_path, url_path, suffix):    \n",
    "    if not os.path.exists(data_path):\n",
    "        os.makedirs(data_path)\n",
    "        \n",
    "    data_path = os.path.join(data_path, f'{suffix}.json')\n",
    "\n",
    "    if not os.path.exists(data_path):\n",
    "        print(f\"Downloading CoQA {suffix} data split... (it may take a while)\")\n",
    "        download_url(url=url_path, output_path=data_path)\n",
    "        print(\"Download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f6ab3ff",
   "metadata": {
    "id": "5f6ab3ff"
   },
   "outputs": [],
   "source": [
    "# Train data\n",
    "train_url = \"https://nlp.stanford.edu/data/coqa/coqa-train-v1.0.json\"\n",
    "download_data(data_path='coqa', url_path=train_url, suffix='train')\n",
    "\n",
    "# Test data\n",
    "test_url = \"https://nlp.stanford.edu/data/coqa/coqa-dev-v1.0.json\"\n",
    "download_data(data_path='coqa', url_path=test_url, suffix='test')  # <-- Why test? See next slides for an answer!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e42311",
   "metadata": {
    "id": "40e42311"
   },
   "source": [
    "#### Data Inspection\n",
    "\n",
    "Spend some time in checking accurately the dataset format and how to retrieve the tasks' inputs and outputs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tUUAc0NYEM-s",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 209,
     "referenced_widgets": [
      "6b0519737d8c4491b69d3c6df4a5c519",
      "8909e8e5f2f54d8bb0cefd59ef802a3c",
      "d7191c4685f94dd9acb935d5f6875118",
      "7737112e6cde4de087c8f9e5a6ec5d61",
      "1044e634e0434d14986687a316bf53a0",
      "8fb9e7e18a2c41a6a554679064be811a",
      "0359681dbee2483bab6a27ed3acff49a",
      "720eb2b49bde42ff9668f0a2d83a0494",
      "7eac541155f8466c8ab758e871b1b0a5",
      "ca5fdbb8962649cd8b44aa6189f35a14",
      "c4f3f36dea1b49c3a5d1fe29426e547e",
      "866903075a5a4faba3eeb03cabb52ea1",
      "5a5a9e4d54b04c3c8f4017eb6a5aa356",
      "afc80424214445388b9557e9d44b4cef",
      "110b22e26654475c9945941995697401",
      "2cd2521bf6a940f1920b99c62d8e2b6c",
      "89a6215ea31c426ebfa53a76e505f5eb",
      "cfc922b403504a0cba807023562d1874",
      "fd607a3be8eb4c199ad7745cce008eeb",
      "af582ac6b439483bb4b28c6ad03c5f52",
      "1e01f77e6f9942e781843d4d5677e6ec",
      "48131b81011a4879a8bdb0e3a8d9e5e7",
      "156b0ba7aaa143c8bff2577f914da428",
      "a81829d0b28d456cbf2f3d79b415ecb2",
      "76785112ba3042dfa848d646f46e2caf",
      "a471a28c0f2844ba9e06d6e1688af6d9",
      "277e13c0a47146849a5ea23f6699db06",
      "0537d1f63b84469ba5e1d68ba28dc705",
      "ad7fedfefdc44a3e9af940c2b13da34c",
      "badccd1b8a244441a99ec98850dfe074",
      "9c74667f30c0420894fe863d07fc32d9",
      "9ed6636d8224420e845c9e16bfed8f65",
      "94adda85c3cf4f14ab615e0c280ca133",
      "71bc02dd3eb44115a5d7d75167acfe3b",
      "3853181aea68478b8e320e379b49873b",
      "47caf508f7b24c1ba30882ef41a8c68a",
      "8d501114e1354c14913cd12887eed765",
      "19df42f9eb2a41e387b5b2e8fcf0046b",
      "a6664d49e15448a5b892596c952dbb1d",
      "5abe54cb350941e7b44f1e344a64ecf2",
      "b402533437824be09721cf8f820585c1",
      "b479b1298372495c961e5a8c6a377e9e",
      "a89a0cec2e8f48869af899b5b15c4625",
      "33b09adae2f24c73b7bff2aee5ab3b9d",
      "5fa8cacb63f94b27a925f6d51a00c209",
      "784ca36bb4b343eebc9049322420981b",
      "83efa302cd8541e584a1a991fa1d331b",
      "52e821ce3416442bb1eb407b7553fd09",
      "09644168005c4c12aefcd12e9c145a46",
      "6f7baafc305c4eb99fd49d017794ddfc",
      "8bc492f63dde4040a0d5b0093f28e4e6",
      "90e2155d9d294ecfbd5ed1cb4e5d7a6d",
      "f3c48eff156e41fb97c39c732c32628f",
      "7430f33e75d949528ac466d518c61979",
      "67f7fe3ad85a4f0094bea0f3adef3f72",
      "7a9921beff1a4c0490089672f06e2943",
      "2941cecad4744271ad885f98ebd5be02",
      "05b45d5259114a86972c561ee279d16b",
      "35ff2258a2c54aec83fee1b35bd67d86",
      "0dc50d96a9564ac3ba24e4d1d13050fe",
      "08977c2ff8b0476a81ac8482d99b684f",
      "fc0ef86fce88409fafc27019e4911d48",
      "70008e8991924dc2b2b9600bd11e9520",
      "554e6bca47bb4310af851b72c2d46185",
      "e74c5807215848aca1c51107f18bfdc9",
      "5174cb4266b44c09ac6031d43d6a754d"
     ]
    },
    "id": "tUUAc0NYEM-s",
    "outputId": "e7f1d1bf-1c47-4de8-947c-9675d4c61c2d"
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import re\n",
    "import string\n",
    "from typing import Callable, Sequence, TypeVar, Tuple\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "from transformers import BertTokenizer, TFBertModel, BertConfig, TFRobertaModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tqdm import tqdm\n",
    "from itertools import islice\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset\n",
    "from numpy.random import seed\n",
    "from tensorflow.random import set_seed\n",
    "\n",
    "\n",
    "def tokenizer(name):\n",
    "    return AutoTokenizer.from_pretrained(name)\n",
    "\n",
    "def nth_index(iterable, value, n):\n",
    "    matches = (idx for idx, val in enumerate(iterable) if val == value)\n",
    "    return next(islice(matches, n-1, n), None)\n",
    "\n",
    "TRAIN_DATA_PATH = 'coqa/train.json'\n",
    "TEST_DATA_PATH = 'coqa/test.json'\n",
    "\n",
    "bert_model_name = 'prajjwal1/bert-tiny'\n",
    "roberta_model_name = 'distilroberta-base'\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "bert_tokenizer = tokenizer(bert_model_name)\n",
    "roberta_tokenizer = tokenizer(roberta_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "MNQk4dQDKHj2",
   "metadata": {
    "id": "MNQk4dQDKHj2"
   },
   "outputs": [],
   "source": [
    "def get_dataframe(file_path, history_memory=0):\n",
    "    with open(file_path) as f:\n",
    "        paragraphs = pd.DataFrame(json.load(f))['data']\n",
    "    titles = ['source', 'id', 'story']  # 'filename', 'name'\n",
    "    data = {k: [] for k in titles +\n",
    "            ['turn_id', 'context', 'question', 'answer', 'span_start', 'span_end', 'span_text']}\n",
    "    for paragraph in paragraphs:\n",
    "        questions = paragraph['questions']\n",
    "        answers = paragraph['answers']\n",
    "        history = []\n",
    "        for i in range(len(questions)):\n",
    "            if answers[i]['input_text'] == 'unknown':\n",
    "                continue\n",
    "            answer = answers[i]['input_text']\n",
    "            question = \" \".join([f\"{history_memory-i} {q} {history_memory-i} {a}.\" for i, (q, a) in enumerate(history)] + [f\"0 {questions[i]['input_text']}\"])\n",
    "            context = paragraph['story']\n",
    "            span_start = answers[i]['span_start']\n",
    "            span_start = span_start + 1 if context[span_start] == ' ' else span_start\n",
    "            while span_start > 0 and context[span_start-1] not in [' ', '\\n', '.', ',']:\n",
    "                span_start -= 1\n",
    "            span_end = answers[i]['span_end']\n",
    "            span_text = context[span_start:span_end]\n",
    "            data['context'].append(context)\n",
    "            data['turn_id'].append(questions[i]['turn_id'])\n",
    "            data['question'].append(question)\n",
    "            data['answer'].append(answer)\n",
    "            data['span_start'].append(span_start)\n",
    "            data['span_end'].append(span_end)\n",
    "            data['span_text'].append(span_text)\n",
    "            for key in titles:\n",
    "                data[key].append(paragraph[key])\n",
    "            history.append((questions[i]['input_text'], answers[i]['input_text']))\n",
    "            if len(history) > history_memory:\n",
    "                history.pop(0)\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wBsh5nN5KKl2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "id": "wBsh5nN5KKl2",
    "outputId": "dd6a8037-f22e-4653-fbfa-3e21f262656f"
   },
   "outputs": [],
   "source": [
    "train = get_dataframe(TRAIN_DATA_PATH)\n",
    "test = get_dataframe(TEST_DATA_PATH)\n",
    "train_h = get_dataframe(TRAIN_DATA_PATH, 2)\n",
    "test_h = get_dataframe(TEST_DATA_PATH, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57334e0",
   "metadata": {
    "id": "f57334e0"
   },
   "source": [
    "## [Task 2] Train, Validation and Test splits\n",
    "\n",
    "CoQA only provides a train and validation set since the test set is hidden for evaluation purposes.\n",
    "\n",
    "We'll consider the provided validation set as a test set. <br>\n",
    "$\\rightarrow$ Write your own script to:\n",
    "* Split the train data in train and validation splits (80% train and 20% val)\n",
    "* Perform splits such that a dialogue appears in one split only! (i.e., split at dialogue level)\n",
    "* Perform splitting using the following seed for reproducibility: 42\n",
    "\n",
    "#### Reproducibility Memo\n",
    "\n",
    "Check back tutorial 2 on how to fix a specific random seed for reproducibility!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "GaQbIR5BkMuI",
   "metadata": {
    "id": "GaQbIR5BkMuI"
   },
   "outputs": [],
   "source": [
    "scale_factor = 2\n",
    "all_ids = train.id.unique()\n",
    "np.random.seed(0)\n",
    "all_ids = all_ids[np.random.choice(all_ids.shape[0], all_ids.shape[0]//scale_factor, replace=False)]\n",
    "train_id_set, val_id_set = train_test_split(all_ids, train_size=.8, random_state=42)\n",
    "\n",
    "def prepare_set_splits(data_split, id_set):\n",
    "    return data_split.loc[data_split.id.isin(id_set)].reset_index()\n",
    "\n",
    "train_set = prepare_set_splits(train, train_id_set)\n",
    "val_set = prepare_set_splits(train, val_id_set)\n",
    "test_set = test\n",
    "\n",
    "train_set_h = prepare_set_splits(train_h, train_id_set)\n",
    "val_set_h = prepare_set_splits(train_h, val_id_set)\n",
    "test_set_h = test_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "gBB4NMbjki0L",
   "metadata": {
    "id": "gBB4NMbjki0L"
   },
   "outputs": [],
   "source": [
    "train_set_d = Dataset.from_pandas(train_set)\n",
    "val_set_d = Dataset.from_pandas(val_set)\n",
    "test_set_d = Dataset.from_pandas(test_set)\n",
    "\n",
    "train_set_d_h = Dataset.from_pandas(train_set_h)\n",
    "val_set_d_h = Dataset.from_pandas(val_set_h)\n",
    "test_set_d_h = Dataset.from_pandas(test_set_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "YjPKfANt_Kke",
   "metadata": {
    "id": "YjPKfANt_Kke"
   },
   "outputs": [],
   "source": [
    "old_cols = train_set_d.features.keys()\n",
    "\n",
    "def prepare_train_features(examples, tokenizer):\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_len,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    offset_mapping = tokenized_examples[\"offset_mapping\"]\n",
    "    tokenized_examples[\"start_positions\"] = []\n",
    "    tokenized_examples[\"end_positions\"] = []\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
    "        \n",
    "        start_char = examples[\"span_start\"][i]\n",
    "        end_char = examples[\"span_end\"][i]\n",
    "        end_of_question = sequence_ids.index(1)\n",
    "        offset = offsets[end_of_question:]\n",
    "        starts, ends = [i for i, j in offset], [j for i, j in offset]\n",
    "        sync = 0\n",
    "        while 1:\n",
    "            try:\n",
    "                last_offset = None\n",
    "                for k in range(1, len(offset)):\n",
    "                    if offset[-k][1] != 0:\n",
    "                        last_offset = offset[-k][1]\n",
    "                        break\n",
    "                if last_offset < start_char + sync or last_offset < end_char:\n",
    "                    raise IndexError\n",
    "                tokenized_examples[\"start_positions\"].append(end_of_question + starts.index(start_char+sync))\n",
    "                break\n",
    "            except ValueError:\n",
    "                sync += 1\n",
    "            except IndexError:\n",
    "                tokenized_examples[\"start_positions\"].append(cls_index)\n",
    "                break\n",
    "        sync = 0\n",
    "        while 1:\n",
    "            try:\n",
    "                last_offset = None\n",
    "                for k in range(1, len(offset)):\n",
    "                    if offset[-k][1] != 0:\n",
    "                        last_offset = offset[-k][1]\n",
    "                        break\n",
    "                if last_offset < start_char or last_offset < end_char + sync:\n",
    "                        raise IndexError\n",
    "                tokenized_examples[\"end_positions\"].append(end_of_question + ends.index(end_char+sync))\n",
    "                break\n",
    "            except ValueError:\n",
    "                sync -= 1\n",
    "            except IndexError:\n",
    "                tokenized_examples[\"end_positions\"].append(cls_index)\n",
    "                break\n",
    "    return tokenized_examples\n",
    "\n",
    "def prepare_validation_features(examples, tokenizer):\n",
    "    pad_on_right = tokenizer.padding_side == \"right\"\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"question\" if pad_on_right else \"context\"],\n",
    "        examples[\"context\" if pad_on_right else \"question\"],\n",
    "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
    "        max_length=max_len,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    return tokenized_examples\n",
    "\n",
    "def get_data(ds, tokenizer):\n",
    "    return ds.map(lambda xx: prepare_train_features(xx, tokenizer), batched=True, remove_columns=old_cols)\n",
    "\n",
    "\n",
    "class XYInputOutput():\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.source = None\n",
    "    \n",
    "    def __hash__(self):\n",
    "        return id(self)\n",
    "\n",
    "    def set_source(self, source):\n",
    "        self.source = source\n",
    "        return self\n",
    "\n",
    "\n",
    "def create_inputs_targets(coqa_examples):\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": [],\n",
    "        \"offset_mapping\": [],\n",
    "        \"token_type_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"start_positions\": [],\n",
    "        \"end_positions\": [],\n",
    "    }\n",
    "    roberta = False\n",
    "    for item in iter(coqa_examples):\n",
    "        for key in dataset_dict:\n",
    "            try:\n",
    "                dataset_dict[key].append(item[key])\n",
    "            except KeyError:\n",
    "                roberta = True\n",
    "    for key in tqdm(dataset_dict):\n",
    "        try:\n",
    "            if roberta and key == 'token_type_ids':\n",
    "                raise KeyError\n",
    "            dataset_dict[key] = np.array(dataset_dict[key])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    x = 0\n",
    "    try:\n",
    "        if roberta:\n",
    "            raise KeyError\n",
    "        x = [\n",
    "        dataset_dict[\"input_ids\"],\n",
    "        dataset_dict[\"token_type_ids\"],\n",
    "        dataset_dict[\"attention_mask\"],\n",
    "        dataset_dict[\"offset_mapping\"],\n",
    "        ]\n",
    "    except KeyError:\n",
    "        x = [\n",
    "        dataset_dict[\"input_ids\"],\n",
    "        dataset_dict[\"attention_mask\"],\n",
    "        dataset_dict[\"offset_mapping\"],\n",
    "        ]\n",
    "    y = [dataset_dict[\"start_positions\"], dataset_dict[\"end_positions\"]]\n",
    "    return XYInputOutput(x, y)\n",
    "\n",
    "def create_inputs(coqa_example):\n",
    "    dataset_dict = {\n",
    "        \"input_ids\": [],\n",
    "        \"attention_mask\": [],\n",
    "        \"token_type_ids\": []\n",
    "    }\n",
    "    roberta = False\n",
    "    for key in dataset_dict:\n",
    "        try:\n",
    "            dataset_dict[key].append(coqa_example[key])\n",
    "        except KeyError:\n",
    "            roberta = True\n",
    "            pass\n",
    "    for key in dataset_dict:\n",
    "        try:\n",
    "            if roberta and key == 'token_type_ids':\n",
    "                raise KeyError\n",
    "            dataset_dict[key] = np.array(dataset_dict[key])\n",
    "        except KeyError:\n",
    "            pass\n",
    "    try:\n",
    "        if roberta:\n",
    "            raise KeyError\n",
    "        x = [\n",
    "            dataset_dict[\"input_ids\"][0],\n",
    "            dataset_dict[\"token_type_ids\"][0],\n",
    "            dataset_dict[\"attention_mask\"][0],\n",
    "        ] \n",
    "    except KeyError:\n",
    "        x = [\n",
    "        dataset_dict[\"input_ids\"][0],\n",
    "        dataset_dict[\"attention_mask\"][0],\n",
    "        ]\n",
    "    return list(np.array(x).reshape((len(x), 1, max_len)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "E4UOndsPlyqL",
   "metadata": {
    "id": "E4UOndsPlyqL"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "def prepare_load_save_all_data_inputs_targets():\n",
    "    result = []\n",
    "    for data, data_name in {\n",
    "#                             train_set_d: \"train\", \\\n",
    "#                             val_set_d: \"val\", \\\n",
    "#                             test_set_d: \"test\", \\\n",
    "                            train_set_d_h: \"train_h\", \\\n",
    "                            val_set_d_h: \"val_h\", \\\n",
    "                            test_set_d_h: \"test_h\" \\\n",
    "                            }.items():\n",
    "        for tokenizer, tokenizer_name in {\n",
    "#                                           bert_tokenizer: \"bert\", \\\n",
    "                                          roberta_tokenizer: \"roberta\" \\\n",
    "                                          }.items():\n",
    "            name = f\"{data_name}_{tokenizer_name}\"\n",
    "            print(f'Creating {name}.')\n",
    "            try:\n",
    "                with open(f\"{name}\", \"rb\") as fp:\n",
    "                    inp_out = pickle.load(fp)\n",
    "            except FileNotFoundError:\n",
    "                inp_out = create_inputs_targets(get_data(data, tokenizer)).set_source(data)\n",
    "                with open(f\"{name}\", \"wb\") as fp:\n",
    "                    pickle.dump(inp_out, fp)\n",
    "            finally:\n",
    "                print(f'{name} Created.')\n",
    "                result.append(inp_out)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CJgU-xpKrQEW",
   "metadata": {
    "id": "CJgU-xpKrQEW"
   },
   "source": [
    "### Resource draining part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "qYJ7PhXggqAU",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170,
     "referenced_widgets": [
      "e478f51e1e8047e888c741b7b5a13244",
      "a6f03eeb313c43f492829d66966e9abe",
      "3292384b6a2b4c62b48dc06ee7fcf5c7",
      "854de8f437cf4171b80b993c10bce051",
      "6f77fc940c824cfe845929aa6cd179ad",
      "1c31073a23bc40e38fd8bd397815ca61",
      "7a31c757c383433390e2e973fc98c54d",
      "5638870ccb6444699937c56820961ec4",
      "ed8602b4418d4998b32935e4fcc46c1e",
      "05f9cdc9fd8f48819186ce3bf86bcc03",
      "04c68dcd281545fe9647d533f2a2766d"
     ]
    },
    "id": "qYJ7PhXggqAU",
    "outputId": "3ecbc7e8-bef4-4109-dd81-271f04793b6e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train_h_roberta.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e671d3ce394f4629b0dfc320dc29ed36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:09<00:00,  1.52s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_h_roberta Created.\n",
      "Creating val_h_roberta.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ec53e24d4c44c088fe23a40c6abda7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_h_roberta Created.\n",
      "Creating test_h_roberta.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f964d750b7164cdea371ac64768d0d43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00,  6.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_h_roberta Created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train_bert\\\n",
    "# train_roberta,\\\n",
    "# val_bert,\\\n",
    "# val_roberta,\\\n",
    "# test_bert,\\\n",
    "# test_roberta,\\\n",
    "# train_bert_h,\\\n",
    "# train_roberta_h,\\\n",
    "# val_bert_h,\\\n",
    "# val_roberta_h,\\\n",
    "# test_bert_h,\\\n",
    "# test_roberta_h,\\\n",
    "train_roberta_h, val_roberta_h, test_roberta_h = prepare_load_save_all_data_inputs_targets()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230a21de",
   "metadata": {
    "id": "230a21de"
   },
   "source": [
    "## [Task 3] Model definition\n",
    "\n",
    "Write your own script to define the following transformer-based models from [huggingface](https://HuggingFace.co/).\n",
    "\n",
    "* [M1] DistilRoBERTa (distilberta-base)\n",
    "* [M2] BERTTiny (bert-tiny)\n",
    "\n",
    "**Note**: Remember to install the ```transformers``` python package!\n",
    "\n",
    "**Note**: We consider small transformer models for computational reasons!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "Wyew3wY1tpve",
   "metadata": {
    "id": "Wyew3wY1tpve"
   },
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    ## encoder\n",
    "    if model_name == roberta_model_name:\n",
    "        encoder = TFRobertaModel.from_pretrained(model_name, from_pt=True)\n",
    "    else:\n",
    "        encoder = TFBertModel.from_pretrained(model_name, from_pt=True)\n",
    "\n",
    "    ## QA Model\n",
    "    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    token_type_ids = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32)\n",
    "    embedding = encoder(\n",
    "        input_ids, \n",
    "        token_type_ids=token_type_ids if model_name == bert_model_name else None, \\\n",
    "        attention_mask=attention_mask\n",
    "    )[0]\n",
    "\n",
    "    start_logits = layers.Dense(1, name=\"start_logit\", use_bias=False)(embedding)\n",
    "    start_logits = layers.Flatten()(start_logits)\n",
    "\n",
    "    end_logits = layers.Dense(1, name=\"end_logit\", use_bias=False)(embedding)\n",
    "    end_logits = layers.Flatten()(end_logits)\n",
    "\n",
    "    start_probs = layers.Activation(keras.activations.softmax)(start_logits)\n",
    "    end_probs = layers.Activation(keras.activations.softmax)(end_logits)\n",
    "\n",
    "    if model_name == bert_model_name:\n",
    "        model = keras.Model(\n",
    "            inputs=[input_ids, token_type_ids, attention_mask],\n",
    "            outputs=[start_probs, end_probs],\n",
    "        )\n",
    "    else:\n",
    "        model = keras.Model(\n",
    "            inputs=[input_ids, attention_mask],\n",
    "            outputs=[start_probs, end_probs],\n",
    "        )\n",
    "    loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
    "    optimizer = keras.optimizers.Adam(lr=5e-5)\n",
    "    model.compile(optimizer=optimizer, loss=[loss, loss])\n",
    "    return model\n",
    "\n",
    "\n",
    "def create_model(model_name, use_tpu=False):\n",
    "    if use_tpu:\n",
    "        # Create distribution strategy\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect()\n",
    "        strategy = tf.distribute.TPUStrategy(tpu)\n",
    "\n",
    "        # Create model\n",
    "        with strategy.scope():\n",
    "            return get_model(model_name)\n",
    "    else:\n",
    "        return get_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "stkqsnNKtr48",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "stkqsnNKtr48",
    "outputId": "abed76ca-8996-49f6-9309-8d910ec1b41e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.LayerNorm.bias', 'bert.embeddings.position_ids', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  4385920     ['input_1[0][0]',                \n",
      "                                thPoolingAndCrossAt               'input_3[0][0]',                \n",
      "                                tentions(last_hidde               'input_2[0][0]']                \n",
      "                                n_state=(None, 512,                                               \n",
      "                                 128),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 128),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " start_logit (Dense)            (None, 512, 1)       128         ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " end_logit (Dense)              (None, 512, 1)       128         ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 512)          0           ['start_logit[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 512)          0           ['end_logit[0][0]']              \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 512)          0           ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 512)          0           ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 4,386,176\n",
      "Trainable params: 4,386,176\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\psole\\.conda\\envs\\tensor\\lib\\site-packages\\keras\\optimizers\\optimizer_v2\\adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "qa_bert = create_model(bert_model_name)\n",
    "qa_bert.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "n3t4E7fb1BiN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3t4E7fb1BiN",
    "outputId": "78d8d318-7836-4d8a-813c-ddd31204843c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " input_6 (InputLayer)           [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_roberta_model (TFRobertaMod  TFBaseModelOutputWi  82118400   ['input_4[0][0]',                \n",
      " el)                            thPoolingAndCrossAt               'input_6[0][0]']                \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 512,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " start_logit (Dense)            (None, 512, 1)       768         ['tf_roberta_model[0][0]']       \n",
      "                                                                                                  \n",
      " end_logit (Dense)              (None, 512, 1)       768         ['tf_roberta_model[0][0]']       \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 512)          0           ['start_logit[0][0]']            \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 512)          0           ['end_logit[0][0]']              \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 512)          0           ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 512)          0           ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 82,119,936\n",
      "Trainable params: 82,119,936\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "qa_roberta = create_model(roberta_model_name)\n",
    "qa_roberta.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e83f28",
   "metadata": {
    "id": "f1e83f28"
   },
   "source": [
    "## [Task 4] Answer generation with text passage $P$ and question $Q$\n",
    "\n",
    "We want to define $f_\\theta(P, Q)$. \n",
    "\n",
    "Write your own script to implement $f_\\theta$ for each model: M1 and M2.\n",
    "\n",
    "#### Formulation\n",
    "\n",
    "Consider a dialogue on text passage $P$. \n",
    "\n",
    "For each question $Q_i$ at dialogue turn $i$, your model should take $P$ and $Q_i$ and generate $A_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "FZAyPa0MpBoU",
   "metadata": {
    "id": "FZAyPa0MpBoU"
   },
   "outputs": [],
   "source": [
    "class color:\n",
    "    PURPLE = '\\033[95m'   \n",
    "    CYAN = '\\033[96m'\n",
    "    DARKCYAN = '\\033[36m'\n",
    "    BLUE = '\\033[94m'\n",
    "    GREEN = '\\033[92m'\n",
    "    YELLOW = '\\033[93m'\n",
    "    RED = '\\033[91m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "qX-xwB3w1dmz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qX-xwB3w1dmz",
    "outputId": "f622e69d-1b06-4d06-ce6b-043547804e70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mBERT:\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m When was the Vat formally opened?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what is the library for?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92me Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m for what subjects?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mthe Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is muc\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m and?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92md the Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what was started in 2014?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m how do scholars divide the library?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m how many?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mthe Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what is the official name of the Vat?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m where is it?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mthe Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is muc\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m how many printed books does it contain?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m when were the Secret Archives moved from the rest of the library?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92me Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of th\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m how many items are in this secret collection?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m Can anyone use this library?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what must be requested in person or by mail?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m of what books?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mthe Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is muc\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m What is the Vat the library of?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m How many books survived the Pre Lateran period?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what is the point of the project started in 2014?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mthe Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what will this allow?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mhe Vatican Library or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much\u001b[0m\n",
      "\u001b[1mRoBERTa:\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m When was the Vat formally opened?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92montains one of the most significant collections of\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what is the library for?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mcontains one of the most significant collections\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m for what subjects?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m and?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what was started in 2014?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mcontains one of the most significant collections\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m how do scholars divide the library?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mcontains one of the most significant collections o\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m how many?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what is the official name of the Vat?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mtains one of the most significant collections of h\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m where is it?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m how many printed books does it contain?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92montains one of the most significant collections of\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m when were the Secret Archives moved from the rest of the library?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mns one of the most significant collections of hist\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m how many items are in this secret collection?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mntains one of the most significant collections of\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m Can anyone use this library?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mcontains one of the most significant collections\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what must be requested in person or by mail?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m of what books?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m What is the Vat the library of?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mntains one of the most significant collections of\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m How many books survived the Pre Lateran period?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mtains one of the most significant collections of h\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what is the point of the project started in 2014?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mains one of the most significant collections of hi\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what will this allow?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def f_theta(passage, question, model, tokenizer):\n",
    "    data = prepare_validation_features({'context': context, 'question': question}, tokenizer)\n",
    "    inputs = create_inputs(data)\n",
    "    pred_start, pred_end = model.predict(inputs, verbose=0)\n",
    "    start = np.argmax(pred_start[0])\n",
    "    end = np.argmax(pred_end[0])\n",
    "    print(f\"\\t{color.BOLD}Question:{color.END} {question}\")\n",
    "    if start >= end:\n",
    "        print(f\"\\t{color.RED}Not able to find an answer.{color.END}\")\n",
    "    else:\n",
    "        print(f\"\\t{color.GREEN}{color.BOLD}Answer: {color.END}{color.GREEN}{passage[start:end].strip()}{color.END}\")\n",
    "\n",
    "\n",
    "dialogue_id = train_set.iloc[0]['id']\n",
    "dialogues = train_set[train_set.id == dialogue_id]\n",
    "print(color.BOLD + 'BERT:' + color.END)\n",
    "for turn_id in dialogues.turn_id.values:\n",
    "    dialogue = dialogues[dialogues.turn_id == turn_id]\n",
    "    context = dialogue.context.values[0]\n",
    "    question = dialogue.question.values[0][2:]\n",
    "    f_theta(context, question, qa_bert, bert_tokenizer)\n",
    "print(color.BOLD + 'RoBERTa:' + color.END)\n",
    "for turn_id in dialogues.turn_id.values:\n",
    "    dialogue = dialogues[dialogues.turn_id == turn_id]\n",
    "    context = dialogue.context.values[0]\n",
    "    question = dialogue.question.values[0][2:]\n",
    "    f_theta(context, question, qa_roberta, roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7311ba86",
   "metadata": {
    "id": "7311ba86"
   },
   "source": [
    "## [Task 5] Answer generation with text passage $P$, question $Q$ and dialogue history $H$\n",
    "\n",
    "We want to define $f_\\theta(P, Q, H)$. Write your own script to implement $f_\\theta$ for each model: M1 and M2.\n",
    "\n",
    "#### Formulation\n",
    "\n",
    "Consider a dialogue on text passage $P$. \n",
    "\n",
    "For each question $Q_i$ at dialogue turn $i$, your model should take $P$, $Q_i$, and $H = \\{ Q_0, A_0, \\dots, Q_{i-1}, A_{i-1} \\}$ to generate $A_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "LBCu1jjBmszI",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LBCu1jjBmszI",
    "outputId": "d0882d4f-9b7d-4bc9-f9a6-adb7c037d9c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mBERT:\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what was started in 2014?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m how do scholars divide the library?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m how many?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what is the official name of the Vat?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m where is it?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92my or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m how many printed books does it contain?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92me Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, alt\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m when were the Secret Archives moved from the rest of the library?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m how many items are in this secret collection?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mis the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most significant colle\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m Can anyone use this library?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much older, it is one of the oldest libraries in the world and contains one of the most si\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what must be requested in person or by mail?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mrary or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although i\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m of what books?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mry or simply the Vat, is the library of the Holy See, located in Vatican City. Formally established in 1475, a\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m What is the Vat the library of?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mt, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m How many books survived the Pre Lateran period?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mt, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what is the point of the project started in 2014?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mVat, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what will this allow?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mt, is the library of the Holy See, located in Vatican City. Formally established in 1475, although it is much\u001b[0m\n",
      "\u001b[1mRoBERTa:\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what was started in 2014?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92most significant collections of historical texts. I\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m how do scholars divide the library?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m how many?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92most significant collections of historical texts. I\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what is the official name of the Vat?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mt significant collections of historical texts. It\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m where is it?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mt significant collections of historical texts. It\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m how many printed books does it contain?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mficant collections of historical texts. It has 75,\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m when were the Secret Archives moved from the rest of the library?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mcant collections of historical texts. It has 75,00\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m how many items are in this secret collection?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m Can anyone use this library?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what must be requested in person or by mail?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m of what books?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m What is the Vat the library of?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m How many books survived the Pre Lateran period?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mnt collections of historical texts. It has 75,000\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what is the point of the project started in 2014?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "\t\u001b[1mQuestion:\u001b[0m what will this allow?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "def f_theta_h(passage, question, history, model, tokenizer):\n",
    "    print(f\"\\t{color.BOLD}Question:{color.END} {question}\")\n",
    "    question = \" \".join([f\"{len(history)-i} {q} {len(history)-i} {a}.\" for i, (q, a) in enumerate(history)] + [f\"0 {question}\"])\n",
    "    data = prepare_validation_features({'context': context, 'question': question}, tokenizer)\n",
    "    inputs = create_inputs(data)\n",
    "    pred_start, pred_end = model.predict(inputs, verbose=0)\n",
    "    start = np.argmax(pred_start[0])\n",
    "    end = np.argmax(pred_end[0])\n",
    "    if start >= end:\n",
    "        print(f\"\\t{color.RED}Not able to find an answer.{color.END}\")\n",
    "    else:\n",
    "        print(f\"\\t{color.GREEN}{color.BOLD}Answer: {color.END}{color.GREEN}{passage[start:end].strip()}{color.END}\")\n",
    "\n",
    "history_memory = 2\n",
    "dialogue_id = train_set.iloc[0]['id']\n",
    "dialogues = train_set[train_set.id == dialogue_id]\n",
    "print(color.BOLD + 'BERT:' + color.END)\n",
    "for turn_id in dialogues.turn_id.values:\n",
    "    if turn_id<5:\n",
    "        continue\n",
    "    dialogue = dialogues[dialogues.turn_id == turn_id]\n",
    "    context = dialogue.context.values[0]\n",
    "    question = dialogue.question.values[0][2:]\n",
    "    history_dialogues = dialogues[dialogues.turn_id < turn_id]\n",
    "    history_dialogues = history_dialogues[history_dialogues.turn_id >= turn_id-history_memory]\n",
    "    history = [(q[2:], a) for q, a in zip(history_dialogues.question.values, history_dialogues.answer.values)]\n",
    "    f_theta_h(context, question, history, qa_bert, bert_tokenizer)\n",
    "print(color.BOLD + 'RoBERTa:' + color.END)\n",
    "for turn_id in dialogues.turn_id.values:\n",
    "    if turn_id<5:\n",
    "        continue\n",
    "    dialogue = dialogues[dialogues.turn_id == turn_id]\n",
    "    context = dialogue.context.values[0]\n",
    "    question = dialogue.question.values[0][2:]\n",
    "    history_dialogues = dialogues[dialogues.turn_id < turn_id]\n",
    "    history_dialogues = history_dialogues[history_dialogues.turn_id >= turn_id-history_memory]\n",
    "    history = [(q[2:], a) for q, a in zip(history_dialogues.question.values, history_dialogues.answer.values)]\n",
    "    f_theta_h(context, question, history, qa_roberta, roberta_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ac768c",
   "metadata": {
    "id": "b5ac768c"
   },
   "source": [
    "## [Task 6] Train and evaluate $f_\\theta(P, Q)$ and $f_\\theta(P, Q, H)$\n",
    "\n",
    "Write your own script to train and evaluate your $f_\\theta(P, Q)$ and $f_\\theta(P, Q, H)$ models.\n",
    "\n",
    "### Instructions\n",
    "\n",
    "* Perform multiple train/evaluation seed runs: [42, 2022, 1337].$^1$\n",
    "* Evaluate your models with the following metrics: SQUAD F1-score.$^2$\n",
    "* Fine-tune each transformer-based models for **3 epochs**.\n",
    "* Report evaluation SQUAD F1-score computed on the validation and test sets.\n",
    "\n",
    "$^1$ Remember what we said about code reproducibility in Tutorial 2!\n",
    "\n",
    "$^2$ You can use ```allennlp``` python package for a quick implementation of SQUAD F1-score: ```from allennlp_models.rc.tools import squad```. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5Bukl1wPumv0",
   "metadata": {
    "id": "5Bukl1wPumv0"
   },
   "outputs": [],
   "source": [
    "def normalize_text(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        regex = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "        return re.sub(regex, \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_text(s).split()\n",
    "\n",
    "\n",
    "def compute_f1(a_pred: str, a_gold: str) -> float:\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    common = collections.Counter(pred_toks) & collections.Counter(gold_toks)  # type: ignore[var-annotated]\n",
    "    num_same = sum(common.values())\n",
    "    if len(pred_toks) == 0 or len(gold_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return float(pred_toks == gold_toks)\n",
    "    if num_same == 0:\n",
    "        return 0.0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1\n",
    "\n",
    "\n",
    "\n",
    "def validate(data, model):\n",
    "    x_eval = data.x[:-1]\n",
    "    y_eval = data.y\n",
    "    source = data.source\n",
    "    offsets_list = data.x[-1]\n",
    "    f1_score = 0\n",
    "    pred_start, pred_end = model.predict(x_eval, verbose = 0)\n",
    "    counter = 0\n",
    "    for idx, (sample, start, end) in enumerate(zip(iter(source), pred_start, pred_end)):\n",
    "        start = np.argmax(start)\n",
    "        end = np.argmax(end)\n",
    "        if start >= end:\n",
    "            continue\n",
    "        offsets = offsets_list[idx]\n",
    "        helper = [i for i, j in np.argwhere(offsets==[0, 0]) if i and j]\n",
    "        start_context = helper[0]\n",
    "        end_context = helper[1]\n",
    "        context = sample['context']\n",
    "        turn_id = sample['turn_id']\n",
    "        question = sample['question']\n",
    "        index = sample['index']\n",
    "        pred_char_start = offsets[start][0]\n",
    "        if end < len(offsets):\n",
    "            pred_char_end = offsets[end][1]\n",
    "            pred_ans = context[pred_char_start:pred_char_end]\n",
    "        else:\n",
    "            pred_ans = context[pred_char_start:]\n",
    "        \n",
    "        true_ans = sample['span_text']\n",
    "        normalized_pred_ans = normalize_text(pred_ans)\n",
    "        normalized_true_ans = normalize_text(true_ans)\n",
    "        sample_f1 = compute_f1(normalized_pred_ans, normalized_true_ans)\n",
    "        if(sample_f1 <= 0.05):\n",
    "            counter += 1\n",
    "            print(counter)\n",
    "            print('turn_id: ', turn_id, '\\nindex: ', index, '\\nQuestion: ', question, '\\npred answer: ', pred_ans, '\\nTrue answer: ', true_ans)\n",
    "            \n",
    "        f1_score += sample_f1\n",
    "    f1_score /= len(y_eval[0])\n",
    "    print(f\"f1_score={f1_score:.5f}\")\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52tdzPi-QKT7",
   "metadata": {
    "id": "52tdzPi-QKT7"
   },
   "outputs": [],
   "source": [
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_yKCvxKRuqLO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_yKCvxKRuqLO",
    "outputId": "692faf6b-6279-4be4-ab93-afaaad0f2b65"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.embeddings.position_ids', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
      "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/214 [==============================] - 53s 165ms/step - loss: 10.5560 - activation_loss: 5.2896 - activation_1_loss: 5.2664 - val_loss: 9.6352 - val_activation_loss: 4.8136 - val_activation_1_loss: 4.8215\n",
      "Epoch 2/3\n",
      "214/214 [==============================] - 11s 49ms/step - loss: 9.6190 - activation_loss: 4.8009 - activation_1_loss: 4.8181 - val_loss: 8.7152 - val_activation_loss: 4.3340 - val_activation_1_loss: 4.3813\n",
      "Epoch 3/3\n",
      "214/214 [==============================] - 11s 50ms/step - loss: 8.8206 - activation_loss: 4.3841 - activation_1_loss: 4.4365 - val_loss: 8.0936 - val_activation_loss: 4.0177 - val_activation_1_loss: 4.0759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.88.247.170:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.embeddings.position_ids', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_1/bert/pooler/dense/kernel:0', 'tf_bert_model_1/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/214 [==============================] - 54s 174ms/step - loss: 10.4933 - activation_2_loss: 5.2494 - activation_3_loss: 5.2439 - val_loss: 9.5954 - val_activation_2_loss: 4.7968 - val_activation_3_loss: 4.7986\n",
      "Epoch 2/3\n",
      "214/214 [==============================] - 10s 49ms/step - loss: 9.5264 - activation_2_loss: 4.7539 - activation_3_loss: 4.7724 - val_loss: 8.5556 - val_activation_2_loss: 4.2565 - val_activation_3_loss: 4.2991\n",
      "Epoch 3/3\n",
      "214/214 [==============================] - 11s 49ms/step - loss: 8.7336 - activation_2_loss: 4.3404 - activation_3_loss: 4.3933 - val_loss: 8.0543 - val_activation_2_loss: 4.0036 - val_activation_3_loss: 4.0507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.88.247.170:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'bert.embeddings.position_ids', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214/214 [==============================] - 52s 163ms/step - loss: 10.5281 - activation_4_loss: 5.2750 - activation_5_loss: 5.2531 - val_loss: 9.6235 - val_activation_4_loss: 4.8147 - val_activation_5_loss: 4.8089\n",
      "Epoch 2/3\n",
      "214/214 [==============================] - 10s 49ms/step - loss: 9.5553 - activation_4_loss: 4.7757 - activation_5_loss: 4.7795 - val_loss: 8.6109 - val_activation_4_loss: 4.3067 - val_activation_5_loss: 4.3041\n",
      "Epoch 3/3\n",
      "214/214 [==============================] - 11s 50ms/step - loss: 8.7766 - activation_4_loss: 4.3708 - activation_5_loss: 4.4058 - val_loss: 8.0946 - val_activation_4_loss: 4.0342 - val_activation_5_loss: 4.0603\n"
     ]
    }
   ],
   "source": [
    "for random_seed in [\n",
    "    # 42,\n",
    "    # 2022,\n",
    "    1337,\n",
    "    ]:\n",
    "    seed(random_seed)\n",
    "    set_seed(random_seed)\n",
    "    for train, val, test, model_name, hist in [\n",
    "        # (train_bert, val_bert, test_bert, bert_model_name, False),\n",
    "        # (train_roberta, val_roberta, test_roberta, roberta_model_name, False),\n",
    "        # (train_bert_h, val_bert_h, test_bert_h, bert_model_name, True)\n",
    "        (train_roberta_h, val_roberta_h, test_roberta_h, roberta_model_name, True),\n",
    "    ]:\n",
    "          model_key = f\"{model_name}_{random_seed}{'_h' if hist else ''}\"\n",
    "          try:\n",
    "              model = models[model_key]\n",
    "          except KeyError:\n",
    "            model = create_model(model_name, True)\n",
    "          model.fit(\n",
    "              train.x[:-1],\n",
    "              train.y,\n",
    "              epochs=3,\n",
    "              verbose=1,\n",
    "              batch_size=200 if model_name == bert_model_name else 32,\n",
    "              validation_data=(val.x[:-1], val.y),\n",
    "          )\n",
    "          models[model_key] = model\n",
    "          # model.save_weights(f\"/content/drive/MyDrive/checkpoints/bert-tiny_{random_seed}{'_h' if hist else ''}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "V_F2KbtE4ycj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V_F2KbtE4ycj",
    "outputId": "a81a629b-8291-4628-e6fe-44ae6b85b02e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MA1-HiR3-bDL",
   "metadata": {
    "id": "MA1-HiR3-bDL"
   },
   "source": [
    "### load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "FF_IkrInQ6FW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FF_IkrInQ6FW",
    "outputId": "5d732691-ea53-404e-91ea-9d106fb25dae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.110.210.250:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
      "/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 14s 91ms/step\n",
      "f1_score=0.48486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.110.210.250:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 14s 89ms/step\n",
      "f1_score=0.47579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.110.210.250:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 14s 90ms/step\n",
      "f1_score=0.47486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.110.210.250:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 14s 89ms/step\n",
      "f1_score=0.54713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.110.210.250:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 14s 90ms/step\n",
      "f1_score=0.54564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.110.210.250:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 14s 91ms/step\n",
      "f1_score=0.53971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.110.210.250:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 7s 44ms/step\n",
      "f1_score=0.21950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.110.210.250:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 8s 45ms/step\n",
      "f1_score=0.16867\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.110.210.250:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 8s 48ms/step\n",
      "f1_score=0.16994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.110.210.250:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 8s 47ms/step\n",
      "f1_score=0.14096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.110.210.250:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 7s 44ms/step\n",
      "f1_score=0.13934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:TPU system grpc://10.110.210.250:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n",
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'bert.embeddings.position_ids']\n",
      "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFBertModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 8s 46ms/step\n",
      "f1_score=0.13400\n",
      "{'distilroberta-base_42': 0.48486468732310956, 'distilroberta-base_2022': 0.47578566677079914, 'distilroberta-base_1337': 0.4748605532484781, 'distilroberta-base_42_h': 0.5471264434644234, 'distilroberta-base_2022_h': 0.5456373762303172, 'distilroberta-base_1337_h': 0.5397110697558715, 'bert-tiny_42': 0.2195032285180981, 'bert-tiny_2022': 0.1686661558906862, 'bert-tiny_1337': 0.1699359185097832, 'bert-tiny_42_h': 0.14095852525173905, 'bert-tiny_2022_h': 0.13933953957878534, 'bert-tiny_1337_h': 0.1339994296646173}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "test_f1_scores = {}\n",
    "pattern = '^.+_h.h5$'\n",
    "weight_files = os.listdir('/content/drive/MyDrive/checkpoints')\n",
    "weight_files.remove('.ipynb_checkpoints')\n",
    "for weight_file in weight_files:\n",
    "    if weight_file.split('-')[0] == 'bert':\n",
    "        model_name = bert_model_name\n",
    "        if (re.match(pattern, weight_file)):\n",
    "            test_set = test_bert_h\n",
    "        else:\n",
    "            test_set = test_bert\n",
    "    else:\n",
    "        model_name = roberta_model_name\n",
    "        if (re.match(pattern, weight_file)):\n",
    "            test_set = test_roberta_h\n",
    "        else:\n",
    "            test_set = test_roberta\n",
    "\n",
    "    model = create_model(model_name, True)\n",
    "    model.load_weights(f\"/content/drive/MyDrive/checkpoints/{weight_file}\")\n",
    "    test_f1_scores[weight_file.split('.')[0]] = validate(test_set, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "Wra9EMHerh6Y",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wra9EMHerh6Y",
    "outputId": "041528f4-eb09-4ffb-d1aa-f620d7687745"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'distilroberta-base_42': 0.48486468732310956, 'distilroberta-base_2022': 0.47578566677079914, 'distilroberta-base_1337': 0.4748605532484781, 'distilroberta-base_42_h': 0.5471264434644234, 'distilroberta-base_2022_h': 0.5456373762303172, 'distilroberta-base_1337_h': 0.5397110697558715, 'bert-tiny_42': 0.2195032285180981, 'bert-tiny_2022': 0.1686661558906862, 'bert-tiny_1337': 0.1699359185097832, 'bert-tiny_42_h': 0.14095852525173905, 'bert-tiny_2022_h': 0.13933953957878534, 'bert-tiny_1337_h': 0.1339994296646173}\n"
     ]
    }
   ],
   "source": [
    "print(test_f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7e98f",
   "metadata": {
    "id": "92c7e98f"
   },
   "source": [
    "## [Task 7] Error Analysis\n",
    "\n",
    "Perform a simple and short error analysis as follows:\n",
    "* Group dialogues by ```source``` and report the worst 5 model errors for each source (w.r.t. SQUAD F1-score).\n",
    "* Inspect observed results and try to provide some comments (e.g., do the models make errors when faced with a particular question type?)$^1$\n",
    "\n",
    "$^1$ Check the [paper](https://arxiv.org/pdf/1808.07042.pdf) for some valuable information about question/answer types (e.g., Table 6, Table 8) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "14d997f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing TFRobertaModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of TFRobertaModel were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 73s 574ms/step\n",
      "f1_score=0.54732\n"
     ]
    }
   ],
   "source": [
    "#load the most promising model\n",
    "model = create_model(roberta_model_name, False)\n",
    "model.load_weights(\"distilroberta-base_42_h.h5\")\n",
    "f1 = validate(test_roberta_h, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "Rm8VzY_NlKMC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rm8VzY_NlKMC",
    "outputId": "6e668a67-9d3e-49fa-f3ad-cb4f7436dc5c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28fcfdc43d5c4d4785f5bac1805096fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 277ms/step\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb29576d56214deb9f3bd39f0ec6524a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 123ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "818e5907213241b48049631b2bd85480",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 726.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 115ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f2b46de8a9402ba530e500302f99f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 122ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "052d3ebcc1e5458091bd7887b5f5ff54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 122ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b871acb7c514f55aaf51372cbff69f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 121ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7380b746aba4a948380dcc209406157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 123ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64cb706f484e43ec957cf6a2d300303b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 113ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d808a83282c3491aa4133beec972e549",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 121ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a78daa57d7044ccb9be74d41c46c7d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 115ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34303ae6aaf44cac8e83f8a198e5c72e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 115ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f21b6d8a2a5c47288d781b817875fb61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 114ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "398fc565ea3f4f7c9464241cb09403a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 113ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e1e009463354d69b1dab4b855776c3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 140ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3daa9a36e8a5471a8dd3a78ec198a0c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 133ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f46c14a1c3fa40c9a0cd812a3aef9a77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 131ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b057f5c6180f4fc896d3737eeef50edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 131ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33cb158136441e6bea97a1fd765aa07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 123ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "697be2be9eb94c3d82d80462aad37190",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 141ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64274cc6742740d6bd7698caef8005d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 133ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f013a32e8c7247f5b96b578cca62893f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 132ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e38335d6e79944a499a61bb353e2779e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 138ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3bd7d21b5cf40f48886ce8043b8986d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 141ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3e2c272ce542a8baaaee24450c3522",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 148ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a95799a81304ec3923e91815c50dabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 121ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_scores = {}\n",
    "sources = test_set.source.unique()\n",
    "grouped_sources = {source: test_set[test_set.source == source] for source in sources}\n",
    "for source, data in grouped_sources.items():\n",
    "    d = []\n",
    "    for i in range(data.shape[0]):\n",
    "        row_data = Dataset.from_pandas(data.iloc[i:i+1])\n",
    "        test = create_inputs_targets(get_data(row_data, roberta_tokenizer)).set_source(row_data)\n",
    "        f1_score = validate(test, model, False)\n",
    "        context = data.context.values[i]\n",
    "        question = data.question.values[i][2:]\n",
    "        d.append([f1_score, context, question, data.span_text.values[i], data.answer.values[i]])\n",
    "    test_scores[source] = d\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "cad9f8c6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8652117aa64bef95edf68c6772912b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:00<00:00, 31.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "race\n",
      "1\n",
      "turn_id:  8 \n",
      "index:  28 \n",
      "Question:  2 Where does Nicole live? 2 Shanghai. 1 How is she related to the boy? 1 mother. 0 What is in the bag? \n",
      "pred answer:  she holds a paper carrier bag \n",
      "True answer:  bag--a thermos with hot soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake\n",
      "2\n",
      "turn_id:  9 \n",
      "index:  29 \n",
      "Question:  2 How is she related to the boy? 2 mother. 1 What is in the bag? 1 food. 0 Has she done this before? \n",
      "pred answer:  It is not her first visit. \n",
      "True answer:  This has become an almost-daily practice. \n",
      "3\n",
      "turn_id:  11 \n",
      "index:  31 \n",
      "Question:  2 Has she done this before? 2 Yes. 1 Why? 1 I am having heart surgery soon, so her mother has decided I need more nutrients. 0 What has helped us communicate? \n",
      "pred answer:  Communication between us is somewhat affected by the fact that she doesn't speak English and all I can say in Chinese is hello \n",
      "True answer:  an iPad\n",
      "4\n",
      "turn_id:  12 \n",
      "index:  32 \n",
      "Question:  2 Why? 2 I am having heart surgery soon, so her mother has decided I need more nutrients. 1 What has helped us communicate? 1 an iPad. 0 What kind of dishes does she bring? \n",
      "pred answer:  Once, she brought an iPad as well as the food \n",
      "True answer:  hot soup and a stainless-steel container with rice, vegetables and either chicken, meat or shrimp, sometimes with a kind of pancake\n",
      "5\n",
      "turn_id:  6 \n",
      "index:  124 \n",
      "Question:  2 What did they do to green tea after picking it? 2 prune it. 1 What good thing do the tea do to the health? 1 may prevent heart disease.. 0 How was the tea created? \n",
      "pred answer:  And the tea bag was born. \n",
      "True answer:  by accident\n",
      "6\n",
      "turn_id:  9 \n",
      "index:  127 \n",
      "Question:  2 Who took the tea first? 2 Shen Nong. 1 When did he take it? 1 about 2737 B.C. 0 Was he happy with it? \n",
      "pred answer:  Shen had bad digestion. \n",
      "True answer:  He liked it.\n",
      "7\n",
      "turn_id:  2 \n",
      "index:  166 \n",
      "Question:  2 Did they go inside the shop? 2 No. 0 What did they end up buying? \n",
      "pred answer:  Shall we buy her a sweater \n",
      "True answer:  flowers\n",
      "8\n",
      "turn_id:  4 \n",
      "index:  168 \n",
      "Question:  2 What did they end up buying? 2 They bought flowers.. 1 How much was a table? 1 It's $15.. 0 Did the purchase it? \n",
      "pred answer:  But they haven't any here. \n",
      "True answer:  It doesn't look good,\n",
      "9\n",
      "turn_id:  8 \n",
      "index:  172 \n",
      "Question:  2 What season will it be soon? 2 summer. 1 How much was the cheap diamond? 1 $15. 0 Was it a real set? \n",
      "pred answer:  A real diamond ring is at least $500 \n",
      "True answer:  They only look like diamonds\n",
      "10\n",
      "turn_id:  9 \n",
      "index:  173 \n",
      "Question:  2 How much was the cheap diamond? 2 $15. 1 Was it a real set? 1 No. 0 What was $10? \n",
      "pred answer:  A real diamond ring is at least $500 \n",
      "True answer:  a pen\n",
      "11\n",
      "turn_id:  10 \n",
      "index:  174 \n",
      "Question:  2 Was it a real set? 2 No. 1 What was $10? 1 a pen. 0 Why didn't the children purchase her a nice shirt? \n",
      "pred answer:  Mum likes chairs \n",
      "True answer:  she has two blouses\n",
      "12\n",
      "turn_id:  12 \n",
      "index:  176 \n",
      "Question:  2 Why didn't the children purchase her a nice shirt? 2 She already has two blouses. 1 What was the occasion for buying a gift? 1 Mother's birthday. 0 What is the cost of a real diamond? \n",
      "pred answer:  The cheapest is $15 \n",
      "True answer:  at least $500\n",
      "13\n",
      "turn_id:  3 \n",
      "index:  189 \n",
      "Question:  2 What happened to the hospital where Hans was? 2 The hospital had been bombed.. 1 Was he with his unit at the time? 1 No.. 0 What country was he in? \n",
      "pred answer:  Hans had made his way back into Western Germany \n",
      "True answer:  German \n",
      "14\n",
      "turn_id:  5 \n",
      "index:  191 \n",
      "Question:  2 What country was he in? 2 Germany. 1 Was he in the Eastern or Western part? 1 Eastern Germany at the time of his hospital stay.. 0 And which part did he need to get to? \n",
      "pred answer:  After having been wondered towards the end of the war, he had been sent to hospital and was separated from his unit \n",
      "True answer:  Western Germany\n",
      "15\n",
      "turn_id:  7 \n",
      "index:  193 \n",
      "Question:  2 And which part did he need to get to? 2 Western Germany. 1 Did he drive there? 1 No. 0 Was the war nearly over by this time? \n",
      "pred answer:  The hospital had been bombed and Hans had made his way back into Western Germany on foot. \n",
      "True answer:  towards the end of the wa\n",
      "16\n",
      "turn_id:  11 \n",
      "index:  197 \n",
      "Question:  2 Did he know what happened to his family? 2 No, just guessed.. 1 What did he decide to do? 1 Hans settled down in a village fifty miles away.. 0 Did he stay there long? \n",
      "pred answer:  Hans settled down in a village fifty miles away where he had remained ever since. \n",
      "True answer:  twenty years\n",
      "17\n",
      "turn_id:  12 \n",
      "index:  198 \n",
      "Question:  2 What did he decide to do? 2 Hans settled down in a village fifty miles away.. 1 Did he stay there long? 1 Yes, for twenty years. 0 What kind of job did he do? \n",
      "pred answer:  A German taxi-driver \n",
      "True answer:  a workman\n",
      "18\n",
      "turn_id:  16 \n",
      "index:  202 \n",
      "Question:  2 What's his name? 2 Hans Bussman. 1 Does he drive a car for a living? 1 Yes, Franz does.. 0 Did the two brothers keep in contact the past twenty years? \n",
      "pred answer:  recently found his brother who was thought to have been killed twenty years ago.  \n",
      "True answer:  When the brothers were reunited\n",
      "19\n",
      "turn_id:  5 \n",
      "index:  230 \n",
      "Question:  2 How did they feel about one another? 2 loved each other. 1 Where did they kill the grass? 1 worn a path through the grass of the field. 0 Who did Brownie belong to? \n",
      "pred answer:  Brownie and Spotty were neighbor dogs \n",
      "True answer:  Ted, Brownie's owner,\n",
      "20\n",
      "turn_id:  6 \n",
      "index:  231 \n",
      "Question:  2 Where did they kill the grass? 2 worn a path through the grass of the field. 1 Who did Brownie belong to? 1 Ted,. 0 Which dog had his leg injured? \n",
      "pred answer:  Brownie's leg was treated by a veterinarian and he recovered. \n",
      "True answer:  Ted found his beloved Brownie alive, one of his hind legs crushed in a steel leghold trap.\n",
      "21\n",
      "turn_id:  7 \n",
      "index:  232 \n",
      "Question:  2 Who did Brownie belong to? 2 Ted,. 1 Which dog had his leg injured? 1 Brownie. 0 Did Spotty go missing? \n",
      "pred answer:  by the next week he was still missing. \n",
      "True answer:  One evening, Brownie's family noticed that Brownie hadn't returned home.\n",
      "22\n",
      "turn_id:  7 \n",
      "index:  273 \n",
      "Question:  2 when? 2 Friday. 1 What time? 1 3pm. 0 whose gun was it? \n",
      "pred answer:  Rick James Lohstroh \n",
      "True answer:  belonged to the boy's mother\n",
      "23\n",
      "turn_id:  12 \n",
      "index:  278 \n",
      "Question:  2 Where did he work? 2 at the University of Texas Medical Branch. 1 Where was the mother? 1 inside the house. 0 was anyone with her? \n",
      "pred answer:  The man and woman shared custody of the children.  \n",
      "True answer:  the 7-year-old\n",
      "24\n",
      "turn_id:  10 \n",
      "index:  351 \n",
      "Question:  2 Where was the film made? 2 China.. 1 Who does the dog love? 1 Ti and his son. 0 Did the dog stay a dog forever? \n",
      "pred answer:  But Dicky still wears the dog around his neck. \n",
      "True answer:  it becomes a doll.\n",
      "25\n",
      "turn_id:  10 \n",
      "index:  800 \n",
      "Question:  2 What were the names of the patients? 2 Mohammed Duhair and Ziad Matouk. 1 How long did Mohammed's surgery take? 1 six hours. 0 Was it successful? \n",
      "pred answer:  Two days later, Matouk received a transplant \n",
      "True answer:  \"We are very satisfied with the results,\"\n",
      "26\n",
      "turn_id:  13 \n",
      "index:  803 \n",
      "Question:  2 What does Skaik hope? 2 that Gaza medical teams do independent kidney transplants, and possibly other organs. 1 What is an issue that complicates this? 1 Funding. 0 What will they do in the meantime? \n",
      "pred answer:  carry out kidney transplants \n",
      "True answer:  go back as volunteers to Gaza\n",
      "27\n",
      "turn_id:  18 \n",
      "index:  808 \n",
      "Question:  2 How long does Ziad expect to be off from work? 2 less than six months. 1 How old is he? 1 42. 0 What about the other patient? \n",
      "pred answer:  Two patients were selected for surgery. \n",
      "True answer:  42\n",
      "28\n",
      "turn_id:  4 \n",
      "index:  814 \n",
      "Question:  2 What weather event caused it? 2 A storm. 1 Anyone else survive? 1 No. 0 Where was he after the crash? \n",
      "pred answer:  No one was alive in this accident except Robinson \n",
      "True answer:  Alone on an island without food, water or gun!\n",
      "29\n",
      "turn_id:  5 \n",
      "index:  1311 \n",
      "Question:  2 where does Geoffrey work? 2 Stanford University. 1 who has three children? 1 my brother. 0 what does cwot mean? \n",
      "pred answer:  three screaming kids face to face \n",
      "True answer:  My summer holidays were a complete waste of time.\n",
      "30\n",
      "turn_id:  5 \n",
      "index:  1331 \n",
      "Question:  2 Who did they visit? 2 Andy's grandparents. 1 Did Andy want to go to bed that night? 1 no. 0 What is Tubby? \n",
      "pred answer:  \"This sounds like a job for Tubby \n",
      "True answer:  Andy tried to move the cat\n",
      "31\n",
      "turn_id:  3 \n",
      "index:  1359 \n",
      "Question:  2 who survived a shark attack at Tasmania coast? 2 Hannah Mighall. 1 who was Syb? 1 Her cousin. 0 how long did it take for the story of this attack to be told? \n",
      "pred answer:  Two years ago \n",
      "True answer:  \"Hannah ,now15\n",
      "32\n",
      "turn_id:  5 \n",
      "index:  1361 \n",
      "Question:  2 how long did it take for the story of this attack to be told? 2 two years. 1 how far was she from shore when this incident happened? 1 60 metres. 0 how long was the shark? \n",
      "pred answer:  Two years ago \n",
      "True answer:  five-metre shark \n",
      "33\n",
      "turn_id:  9 \n",
      "index:  1365 \n",
      "Question:  2 what else Syb do to extricate Hannah from the shark's mouth? 2 reached out to grab Hannah. 1 did he rescue her and how? 1 Yes, Hannah dog paddled to Syb's board and he dragged her from the water to lie on his back. 0 why Hannah did not have any hatred for the shark? \n",
      "pred answer:  the fact that great whites,despite their reputation as man-eaters, typically don't target humans \n",
      "True answer:  \"I was in his territory, she wasn't in mine,\"\n",
      "34\n",
      "turn_id:  13 \n",
      "index:  1399 \n",
      "Question:  2 What? 2 first-degree murder. 1 According to what publication? 1 Aurora Sentinel. 0 How old was Caden? \n",
      "pred answer:  2-year-old grandson \n",
      "True answer:  2\n",
      "35\n",
      "turn_id:  14 \n",
      "index:  1400 \n",
      "Question:  2 According to what publication? 2 Aurora Sentinel. 1 How old was Caden? 1 Two. 0 Where was Mark flying from? \n",
      "pred answer:  Southwest Airlines \n",
      "True answer:  Los Angeles\n",
      "36\n",
      "turn_id:  15 \n",
      "index:  1401 \n",
      "Question:  2 How old was Caden? 2 Two. 1 Where was Mark flying from? 1 Los Angeles. 0 Why did he want to go to Denver? \n",
      "pred answer:  \"I was kind of panicking because I was running late, and I really thought I wasn't going to make the flight,\" Dickinson told KABC. \n",
      "\n",
      "That's when a pilot from Southwest Airlines stepped up and held the flight at the gate until Dickinson arrived. The pilot was standing by the air bridge waiting for him when Dickinson arrived in socks, so rushed that he just grabbed his shoes at security and ran through the terminal. \n",
      "\n",
      "\"I told him, 'Thank you so much. I can't tell you how much I appreciated that.' And he said, 'No problem. They can't leave without me anyway,'\"Dickinson told KABC. \n",
      "\n",
      "Authorities say Dickinson's grandson, Caden Rodgers, suffered a head injury after his mother's boyfriend threw him across the room. The boyfriend reportedly told police he was drunk and high on marijuana at the time. The child later died and the boyfriend has been charged with first-degree murder, according to the Aurora Sentinel. \n",
      "\n",
      "Thanks to the pilot, Dickinson made it to Colorado in time to say goodbye to his grandson. \n",
      "True answer:  see his dying 2-year-old grandson\n",
      "37\n",
      "turn_id:  5 \n",
      "index:  1586 \n",
      "Question:  2 What was he called? 2 Peter. 1 Where did he go? 1 to a different school. 0 What did Bob do? \n",
      "pred answer:  Bob wanted to meet Peter \n",
      "True answer:  He asked another student,\n",
      "38\n",
      "turn_id:  1 \n",
      "index:  1612 \n",
      "Question:  0 Meyer grew up where ? \n",
      "pred answer:  He grew up in a modest home \n",
      "True answer:  Universal Parks & Resorts.\n",
      "39\n",
      "turn_id:  2 \n",
      "index:  1613 \n",
      "Question:  2 Meyer grew up where ? 2 Universal Parks & Resorts.. 0 where is he the president ? \n",
      "pred answer:  Ron Meyer is the president and COO \n",
      "True answer:  Universal Studios\n",
      "40\n",
      "turn_id:  3 \n",
      "index:  1614 \n",
      "Question:  2 Meyer grew up where ? 2 Universal Parks & Resorts.. 1 where is he the president ? 1 Universal Studios. 0 hom many multi million dollar productions he did ? \n",
      "pred answer:  He's the guy who oversees the production of Multimillion-dollar extravaganzas \n",
      "True answer:  likeKing KongandCinderella Man\n",
      "41\n",
      "turn_id:  5 \n",
      "index:  1731 \n",
      "Question:  2 his age? 2 38. 1 his profession? 1 pilot. 0 what kind of aircraft was he flying? \n",
      "pred answer:  a plane \n",
      "True answer:  Jian-10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n",
      "turn_id:  4 \n",
      "index:  1797 \n",
      "Question:  2 What is Sandy's last name 2 Lin. 1 what is wrong with her mother? 1 she's ill. 0 can she work? \n",
      "pred answer:  she can't do a part-time job after class. \n",
      "True answer:  I've been ill in bed several years\n",
      "43\n",
      "turn_id:  11 \n",
      "index:  1804 \n",
      "Question:  2 Who did Rose talk to ? 2 Justin. 1 what is his occupation? 1 social worker. 0 does her friends think Sandy can handle her job and school? \n",
      "pred answer:  I'll manage it as soon as I can. \n",
      "True answer:  but it's her time to study hard to enter a good senior high school, she can't do a part-time job after class\n",
      "44\n",
      "turn_id:  2 \n",
      "index:  1831 \n",
      "Question:  2 What defeats all? 2 Labor. 0 What kind of labor? \n",
      "pred answer:  Labor defeats all--not inconstant, or ill-directed labor \n",
      "True answer:  faithful, persistent, daily effort toward a well-directed purpose\n",
      "45\n",
      "turn_id:  10 \n",
      "index:  1839 \n",
      "Question:  2 Like what? 2 science. 1 And? 1 literature. 0 Not math? \n",
      "pred answer:  The celebrated mathematician, Edmund Stone, would never have published a mathematical dictionary \n",
      "True answer:  that guard the entrances to the professions, to science, art, literature, agriculture--every department of human endeavor\n",
      "46\n",
      "turn_id:  20 \n",
      "index:  1848 \n",
      "Question:  2 Did he become well known for something? 2 astronomer. 1 What do you think the last word of the motto is at the beginning? 1 die. 0 Is it a good motto for lazy folks? \n",
      "pred answer:  Labor defeats all--not inconstant, or ill-directed labor; \n",
      "True answer:  \"If I rest, I _ \"----would be an excellent motto for those who are troubled by the slightest bit of idleness\n",
      "47\n",
      "turn_id:  2 \n",
      "index:  1917 \n",
      "Question:  2 who had ALS? 2 I do. 0 who did she take to Kleinfeld? \n",
      "pred answer:  My sister, Stephanie \n",
      "True answer:  Marina\n",
      "48\n",
      "turn_id:  6 \n",
      "index:  1921 \n",
      "Question:  2 was she excited? 2 No. 1 what was she more excited about? 1 getting a tattoo. 0 about what? \n",
      "pred answer:  Marina was more excited about getting a tattoo on her ankle \n",
      "True answer:  to represent my fight with ALS\n",
      "49\n",
      "turn_id:  3 \n",
      "index:  1938 \n",
      "Question:  2 What is a common saying that parents tell their kids? 2 \"Act your age\". 1 Do scientists think it's accurate for parents to say this? 1 yes. 0 Why do teens act immature when they appear to be grown up? \n",
      "pred answer:  While teenagers can look all grown up, studies have shown that their brains are still developing. \n",
      "True answer:  He says considerable development continues in young people from the teenage years into the twenties\n",
      "50\n",
      "turn_id:  1 \n",
      "index:  2165 \n",
      "Question:  0 Is ChiChi a person? \n",
      "pred answer:  ChiChi weighs only 13 pounds. \"He's so tiny,I can carry him with one hand \n",
      "True answer:  the dog\n",
      "51\n",
      "turn_id:  2 \n",
      "index:  2166 \n",
      "Question:  2 Is ChiChi a person? 2 no. 0 What is he? \n",
      "pred answer:  ChiChi weighs only 13 pounds. \"He's so tiny \n",
      "True answer:  the dog\n",
      "52\n",
      "turn_id:  10 \n",
      "index:  2174 \n",
      "Question:  2 Where? 2 in a beach chair.. 1 What were his owners doing? 1 relaxing on the beach. 0 Did the animal do something heroic? \n",
      "pred answer:  The Lanes rushed across the sand and into the surf. \n",
      "True answer:  October,ChiChi proved to be more than just a pretty face\n",
      "53\n",
      "turn_id:  14 \n",
      "index:  2178 \n",
      "Question:  2 Was it an emergency? 2 yes. 1 Was it life-threatening? 1 yes. 0 Who did the animal alert? \n",
      "pred answer:  Rick went to the woman in danger of drowning \n",
      "True answer:  The Lanes sat up\n",
      "54\n",
      "turn_id:  10 \n",
      "index:  2583 \n",
      "Question:  2 Does she have a large family? 2 yes. 1 Who does it consist of? 1 her parents, a brother and two sisters. 0 Who's job did she do? \n",
      "pred answer:  Lin took turns with her mom holding the baby late into the night. \n",
      "True answer:  \"I almost did everything that was supposed to be a parent's job, but I felt proud of myself,\" she said.\n",
      "55\n",
      "turn_id:  20 \n",
      "index:  2593 \n",
      "Question:  2 Where does she go to school? 2 Shenzhen University. 1 How old is her sibling? 1 two. 0 Is it a girl or boy? \n",
      "pred answer:  a younger brother and two younger sisters. Lin took her role as the \"big sister\" seriously. When her first sister was born, Lin took turns with her mom holding the baby late into the night. \"I almost did everything that was supposed to be a parent's job, but I felt proud of myself,\" she said. Not all of Lin's classmates shared her experiences because of the family planning policy , a law that was carried out in China about forty years ago. But the situation is about to change. A new policy made on October 29 said all couples can now have two children. The new policy is expected to help raise the population of China. Recently, China has been facing an aging population and labor shortages because of the low birth rate . The two-child policy may also be the gift for every only child. Researchers found that growing up with a brother or sister can have good influence on one's mental health. \"Brothers and sisters do matter in unique ways. They give kids something that parents don't.\" Laura Walker, a professor from Beijing University, told China Daily. She noted that having a brother or a sister \n",
      "True answer:  Liu Fang, 20, from Shenzhen University, has a 2-year-younger sister.\n",
      "56\n",
      "turn_id:  21 \n",
      "index:  2594 \n",
      "Question:  2 How old is her sibling? 2 two. 1 Is it a girl or boy? 1 a girl. 0 What is the best thing about having a sister? \n",
      "pred answer:  protects teenagers against loneliness, fear \n",
      "True answer:  \"The best part of having a sister is that you have a partner, friend and supporter throughout your life.\" \n",
      "57\n",
      "turn_id:  2 \n",
      "index:  2647 \n",
      "Question:  2 What was the father asked to build? 2 a boat. 0 Who asked? \n",
      "pred answer:  'Daddy, will you build a boat for me?' \n",
      "True answer:  ----a four --year-- old son\n",
      "58\n",
      "turn_id:  9 \n",
      "index:  2744 \n",
      "Question:  2 In Arthur's story, what did the wind sound like? 2 the barking  of Mike's dog Toby. 1 In the story why did Mike feel guilty? 1 Yes. 0 What happened right after Mike let Toby in the door? \n",
      "pred answer:  Then Mike realized that it wasn't the sound of the wind. It was Toby. The dog was running after him down the busy street, barking. The sound was filled with blame. Mike felt guilty because he had left Toby outside. \n",
      "True answer:  the rain started to pour down. \" \n",
      "59\n",
      "turn_id:  7 \n",
      "index:  2884 \n",
      "Question:  2 Why? 2 He feels sorry for himself.. 1 Is today normal or unusual? 1 normal. 0 Where did he see the things he wanted? \n",
      "pred answer:  He really wants to have them for his birthday. \n",
      "True answer:  Tom walks past the shop at the street corner.\n",
      "60\n",
      "turn_id:  18 \n",
      "index:  2895 \n",
      "Question:  2 Does it make his more upset? 2 no. 1 How does he react? 1 He smiles.. 0 Why? \n",
      "pred answer:  He smiles and he is happier. \n",
      "True answer:  \"It's much better to be without shoes than without feet,\"\n",
      "61\n",
      "turn_id:  8 \n",
      "index:  2905 \n",
      "Question:  2 Are they related? 2 Yes. 1 How? 1 They were brothers. 0 How old was the baby? \n",
      "pred answer:  Andrew Willis, 15 \n",
      "True answer:  Her six-month-old son\n",
      "62\n",
      "turn_id:  13 \n",
      "index:  2910 \n",
      "Question:  2 Did the boys call instead? 2 Yes. 1 Who came to thank them? 1 The mother, the baby, and his grandmother. 0 Was there someone who didn't help? \n",
      "pred answer:  The three teenagers rushed to the aid of Ms Price \n",
      "True answer:  \"While I was shouting for help, a woman walking her dog went straight past, without stopping.\n",
      "63\n",
      "turn_id:  14 \n",
      "index:  2911 \n",
      "Question:  2 Who came to thank them? 2 The mother, the baby, and his grandmother. 1 Was there someone who didn't help? 1 Yes. 0 Who? \n",
      "pred answer:  The three teenagers rushed to the aid of Ms Price, called an ambulance , calmed her down and waited with her until the ambulance arrived before heading to school. \n",
      "\n",
      "Ms Price, Corey and his grandmother Joyce Finnie \n",
      "True answer:  \"While I was shouting for help, a woman walking her dog went straight past,\n",
      "64\n",
      "turn_id:  6 \n",
      "index:  2954 \n",
      "Question:  2 what happened to his body? 2 suffered injuries. 1 to what part of his body? 1 spine. 0 how old was he? \n",
      "pred answer:  This 15-year-old young man \n",
      "True answer:  15\n",
      "65\n",
      "turn_id:  11 \n",
      "index:  2959 \n",
      "Question:  2 what for? 2 raise awareness and money. 1 for what? 1 for spinal cord research. 0 how far did he reach? \n",
      "pred answer:  he wheeled 24,901.55 miles \n",
      "True answer:  34 countries\n",
      "66\n",
      "turn_id:  7 \n",
      "index:  3033 \n",
      "Question:  2 Did the phone work? 2 Yes. 1 Who did they call? 1 The mountain service. 0 Did someone get hurt? \n",
      "pred answer:  They were lucky! \n",
      "True answer:  Paul fell on some rocks.He shouted.\"Judy! I've hurt my leg!\n",
      "67\n",
      "turn_id:  14 \n",
      "index:  3040 \n",
      "Question:  2 What did the man on the phone say to do? 2 Not move. 1 Is the mountain a safe place to be? 1 No. 0 Had the two gone uphill? \n",
      "pred answer:  They were not far from the path \n",
      "True answer:  \"Just keep going downhill\n",
      "68\n",
      "turn_id:  15 \n",
      "index:  3041 \n",
      "Question:  2 Is the mountain a safe place to be? 2 No. 1 Had the two gone uphill? 1 No. 0 Which way, then? \n",
      "pred answer:  Then Paul fell on some rocks \n",
      "True answer:  downhill\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69\n",
      "turn_id:  1 \n",
      "index:  3113 \n",
      "Question:  0 What was the name of the great author? \n",
      "pred answer:  Vladimir Ilyich Tolstoy \n",
      "True answer:  This year marks the 100thanniversary of Leo Tolstoy's death. He is considered by many to be one of the greatest novelists of all time.\n",
      "70\n",
      "turn_id:  16 \n",
      "index:  3145 \n",
      "Question:  2 Did the narrator hear something whilst waiting at the elevator? 2 yes. 1 Was it Peter? 1 yes. 0 What did he ask him? \n",
      "pred answer:  Then he asked me to broadcast an imaginary game \n",
      "True answer:  \"What did you say about sports? Do you know anything about football?\" \n",
      "71\n",
      "turn_id:  17 \n",
      "index:  3146 \n",
      "Question:  2 Was it Peter? 2 yes. 1 What did he ask him? 1 what he said about sports. 0 What sports? \n",
      "pred answer:  Then he asked me to broadcast an imaginary game \n",
      "True answer:  Do you know anything about football\n",
      "72\n",
      "turn_id:  3 \n",
      "index:  3236 \n",
      "Question:  2 What is the primary device mentioned here? 2 lifts,or elevators. 1 What can you do when you're in one alone? 1 whatever you want. 0 Do two folks generally hang out close to each other? \n",
      "pred answer:  Usually when we meet other people we have about an arm's length of distance between us. \n",
      "True answer:  you take different corners\n",
      "73\n",
      "turn_id:  5 \n",
      "index:  3238 \n",
      "Question:  2 Do two folks generally hang out close to each other? 2 you take different corners. 1 What about three folks? 1 you will unconsciously form a triangle. 0 And four? \n",
      "pred answer:  A fifth person is probable going to have to stand in the middle.  \n",
      "True answer:  a square\n",
      "74\n",
      "turn_id:  7 \n",
      "index:  3240 \n",
      "Question:  2 And four? 2 a square. 1 What happens to a 5th? 1 stand in the middle. 0 Why is it weirder than in other social places? \n",
      "pred answer:  But perhaps there is more to it than just social awkwardness.  \n",
      "True answer:  \"You don't have enough space\n",
      "75\n",
      "turn_id:  11 \n",
      "index:  3244 \n",
      "Question:  2 Are there any other reasons it's uncomfortable? 2 we are a little anxious. 1 Who said that? 1 Nick White. 0 What can one of these devices feel like? \n",
      "pred answer:  it's a frightening place to be.\"  \n",
      "True answer:  space-a tomb\n",
      "76\n",
      "turn_id:  10 \n",
      "index:  3305 \n",
      "Question:  2 What is in the first drawing? 2 an animal. 1 What's in it's hands? 1 apples. 0 Does the woman in the second picture have brown hair? \n",
      "pred answer:  It is black and white \n",
      "True answer:  She has straight blonde hair\n",
      "77\n",
      "turn_id:  6 \n",
      "index:  3350 \n",
      "Question:  2 Which legendary figure flew to the moon? 2 Chang'e. 1 Who was her spouse? 1 Hou Yi. 0 What is he famous for? \n",
      "pred answer:  People see Hou Yi as a great hero. \n",
      "True answer:  shot down nine suns\n",
      "78\n",
      "turn_id:  10 \n",
      "index:  3354 \n",
      "Question:  2 How many mentions of a full moon being representing good fortune are there in the story? 2 One. 1 What specific event is said to be lucky? 1 If a baby is born on a full moon day. 0 Which deity is responsible for this? \n",
      "pred answer:  Indian people believe he or she is lucky. \n",
      "True answer:  Soma.\n",
      "79\n",
      "turn_id:  12 \n",
      "index:  3356 \n",
      "Question:  2 Which deity is responsible for this? 2 Soma.. 1 What country is he associated with? 1 India. 0 Which deity is famous as a hunter? \n",
      "pred answer:  the god of the moon is Soma. \n",
      "True answer:  Artemis\n",
      "80\n",
      "turn_id:  11 \n",
      "index:  3484 \n",
      "Question:  2 did she write it down right away? 2 no. 1 when did she? 1 later. 0 why? \n",
      "pred answer:  \"like a message from above \n",
      "True answer:  \"I thought it was beautiful\n",
      "81\n",
      "turn_id:  6 \n",
      "index:  3629 \n",
      "Question:  2 Where? 2 Carnegie Mellon. 1 What does he make? 1 robots. 0 Why? \n",
      "pred answer:  The Carnegie Mellon machines are designed to carry cameras and electronic sensors \n",
      "True answer:  developing snake-like robots he hopes will eventually slide through fallen buildings in search of victims trapped after natural disasters or other emergencies. \n",
      "82\n",
      "turn_id:  1 \n",
      "index:  3657 \n",
      "Question:  0 How old is Catherine? \n",
      "pred answer:  54-year-old secretary \n",
      "True answer:  54\n",
      "83\n",
      "turn_id:  3 \n",
      "index:  3659 \n",
      "Question:  2 How old is Catherine? 2 54. 1 where does she live? 1 Sweden. 0 Who is Tom? \n",
      "pred answer:  Tom goes everywhere with Catherine Green, a 54-year-old secretary \n",
      "True answer:  a dog\n",
      "84\n",
      "turn_id:  5 \n",
      "index:  3661 \n",
      "Question:  2 Who is Tom? 2 a dog. 1 Who provides care for its people? 1 the government. 0 is it expensive? \n",
      "pred answer:  However, most such treatment is expensive \n",
      "True answer:  This level of care costs money. \n",
      "85\n",
      "turn_id:  13 \n",
      "index:  3669 \n",
      "Question:  2 What happens if a dog is hit by a car? 2 the owner, has to pay for any damage done to the car,. 1 but what if the dog is killed? 1 even if your dog has been killed in the accident.. 0 What does Catherine do for work? \n",
      "pred answer:  He moves around her office at work and goes shopping with her. \n",
      "True answer:  secretary\n",
      "86\n",
      "turn_id:  13 \n",
      "index:  3737 \n",
      "Question:  2 how did William react to her message? 2 he cried. 1 how old was she when he wrote to her? 1 her eighties. 0 how did the note make her feel? \n",
      "pred answer:  very warm \n",
      "True answer:  I can't tell you how much your letter meant to me\n",
      "87\n",
      "turn_id:  4 \n",
      "index:  3872 \n",
      "Question:  2 and how much does he weigh? 2 200 pounds.. 1 does he keep track of all the details about the children? 1 Yes.. 0 is he a robot? \n",
      "pred answer:  Mr. Leachim, who weights two hundred pounds and is six feet tall, has some advantages as a teacher \n",
      "True answer:  His computer brain \n",
      "f1_score=0.51310\n"
     ]
    }
   ],
   "source": [
    "test_scores = {}\n",
    "sources = test_set_h.source.unique()\n",
    "grouped_sources = {source: test_set_h[test_set_h.source == source] for source in sources}\n",
    "\n",
    "for i, (source, data) in enumerate(grouped_sources.items()):\n",
    "    if i != 1:\n",
    "        continue\n",
    "    data = Dataset.from_pandas(data)\n",
    "    test = create_inputs_targets(get_data(data, roberta_tokenizer)).set_source(data)\n",
    "    print(source)\n",
    "    validate(test, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "78472309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mctest\n",
      "\t\u001b[1mQuestion:\u001b[0m Who did she live with?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92ma ni\u001b[0m\n",
      "with her mommy and 5 other sisters\n",
      "\t\u001b[1mQuestion:\u001b[0m What color was Cotton?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mnear a farm house, there lived a little white kitten n\u001b[0m\n",
      "a little white kitten named Cotton\n",
      "\t\u001b[1mQuestion:\u001b[0m What color were her sisters?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mar a farm house, there lived a little white kitten named Cotton. Cotton lived high up in a ni\u001b[0m\n",
      "her sisters were all orange with beautiful white tiger stripes\n",
      "\t\u001b[1mQuestion:\u001b[0m Where did she live?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mar\u001b[0m\n",
      "in a barn near a farm house, there lived a little white kitten\n",
      "\t\u001b[1mQuestion:\u001b[0m Did she live alone?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "Cotton wasn't alone\n",
      "race\n",
      "\t\u001b[1mQuestion:\u001b[0m Who is at the door?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mp, I\u001b[0m\n",
      "On the step, I find the elderly Chinese lady, small and slight, holding the hand of a little boy\n",
      "\t\u001b[1mQuestion:\u001b[0m What?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "a paper carrier bag\n",
      "\t\u001b[1mQuestion:\u001b[0m Is she carrying something?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "she holds a paper carrier bag\n",
      "\t\u001b[1mQuestion:\u001b[0m Do I know her?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "I know this lady\n",
      "\t\u001b[1mQuestion:\u001b[0m Who is her daughter?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mp, I find the elderly Chinese lady, small and slight, holding the hand of a little boy. In her other hand, she holds a paper carrier bag. \n",
      "\n",
      "I know this lady. It is not her first visit. She is the boy's grandmother, and her daught\u001b[0m\n",
      "Her daughter, Nicole\n",
      "cnn\n",
      "\t\u001b[1mQuestion:\u001b[0m Is someone in showbiz?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "Dennis Farina, the dapper, mustachioed cop-turned-actor best known for his tough-as-nails work in such TV series as \"Law & Order,\" \"Crime Story,\" and \"Miami Vice,\" has died\n",
      "\t\u001b[1mQuestion:\u001b[0m Whom?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92me da\u001b[0m\n",
      "Dennis Farina, the dapper, mustachioed cop-turned-actor best known for his tough-as-nails work in such TV series as \"Law & Order,\" \"Crime Story,\" and \"Miami Vice,\" has died. \n",
      "\t\u001b[1mQuestion:\u001b[0m Was he in movies?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "Farina, who had a long career as a police officer in Chicago, got into acting through director Michael Mann, who used him as a consultant and cast him in his 1981 movie, \"Thief.\"\n",
      "\t\u001b[1mQuestion:\u001b[0m What did he do?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mown\u001b[0m\n",
      "Dennis Farina, the dapper, mustachioed cop-turned-actor b\n",
      "\t\u001b[1mQuestion:\u001b[0m Is he still alive?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92m(CNN\u001b[0m\n",
      "Dennis Farina, the dapper, mustachioed cop-turned-actor best known for his tough-as-nails work in such TV series as \"Law & Order,\" \"Crime Story,\" and \"Miami Vice,\" has died\n",
      "wikipedia\n",
      "\t\u001b[1mQuestion:\u001b[0m and state?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "New York\n",
      "\t\u001b[1mQuestion:\u001b[0m How many burroughs are there?\n",
      "\t\u001b[91mNot able to find an answer.\u001b[0m\n",
      "five\n",
      "\t\u001b[1mQuestion:\u001b[0m Is staten island one?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92ms one of the\u001b[0m\n",
      "one \n",
      "\t\u001b[1mQuestion:\u001b[0m Where is it?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mis one of t\u001b[0m\n",
      "York. In the southwest of the city,\n",
      "\t\u001b[1mQuestion:\u001b[0m in what city?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mt\u001b[0m\n",
      "New York City\n",
      "gutenberg\n",
      "\t\u001b[1mQuestion:\u001b[0m What lay between the shore-reefs and outer-reefs?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mt of Mal\u001b[0m\n",
      "threading the colour-riotous lagoon that lay between the shore-reefs and outer-reefs\n",
      "\t\u001b[1mQuestion:\u001b[0m Did he have red hair?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mher le\u001b[0m\n",
      "Captain Winters averred each day added a thousand grey hairs\n",
      "\t\u001b[1mQuestion:\u001b[0m What worked her way northward?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mg the leew\u001b[0m\n",
      "Northward, along the leeward coast of Malaita, the _Ariel_ worked her leisurely way\n",
      "\t\u001b[1mQuestion:\u001b[0m Were the passages wide?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92mthe\u001b[0m\n",
      "daring passages so narrow \n",
      "\t\u001b[1mQuestion:\u001b[0m Who was the Captain?\n",
      "\t\u001b[92m\u001b[1mAnswer: \u001b[0m\u001b[92m_ wo\u001b[0m\n",
      "Captain Winters\n"
     ]
    }
   ],
   "source": [
    "#Print 5 worst predicted answers with respect to f1-score for each source\n",
    "for source, scores in test_scores.items():\n",
    "    print(source)\n",
    "    scores.sort(key=lambda row: (row[0]), reverse=False)\n",
    "    for score in scores[:6]:\n",
    "        f_theta(score[1], score[2], model, roberta_tokenizer)\n",
    "        print(score[3])\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1814004",
   "metadata": {
    "id": "f1814004"
   },
   "source": [
    "# Assignment Evaluation\n",
    "\n",
    "The following assignment points will be awarded for each task as follows:\n",
    "\n",
    "* Task 1, Pre-processing $\\rightarrow$ 0.5 points.\n",
    "* Task 2, Dataset Splitting $\\rightarrow$ 0.5 points.\n",
    "* Task 3 and 4, Models Definition $\\rightarrow$ 1.0 points.\n",
    "* Task 5 and 6, Models Training and Evaluation $\\rightarrow$ 2.0 points.\n",
    "* Task 7, Analysis $\\rightarrow$ 1.0 points.\n",
    "* Report $\\rightarrow$ 1.0 points.\n",
    "\n",
    "**Total** = 6 points <br>\n",
    "\n",
    "We may award an additional 0.5 points for outstanding submissions. \n",
    " \n",
    "**Speed Bonus** = 0.5 extra points <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a1b2b9",
   "metadata": {
    "id": "20a1b2b9"
   },
   "source": [
    "# Report\n",
    "\n",
    "We apply the rules described in Assignment 1 regarding the report.\n",
    "* Write a clear and concise report following the given overleaf template (**max 2 pages**).\n",
    "* Report validation and test results in a table.$^1$\n",
    "* **Avoid reporting** code snippets or copy-paste terminal outputs $\\rightarrow$ **Provide a clean schema** of what you want to show"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0967c209",
   "metadata": {
    "id": "0967c209"
   },
   "source": [
    "# Comments and Organization\n",
    "\n",
    "Remember to properly comment your code (it is not necessary to comment each single line) and don't forget to describe your work!\n",
    "\n",
    "Structure your code for readability and maintenance. If you work with Colab, use sections. \n",
    "\n",
    "This allows you to build clean and modular code, as well as easy to read and to debug (notebooks can be quite tricky time to time)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23929660",
   "metadata": {
    "id": "23929660"
   },
   "source": [
    "# FAQ (READ THIS!)\n",
    "\n",
    "---\n",
    "\n",
    "**Question**: Does Task 3 also include data tokenization and conversion step?\n",
    "\n",
    "**Answer:** Yes! These steps are usually straightforward since ```transformers``` also offers a specific tokenizer for each model.\n",
    "\n",
    "**Example**: \n",
    "\n",
    "```\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "encoded_text = tokenizer(text)\n",
    "%% Alternatively\n",
    "inputs = tokenizer.tokenize(text, add_special_tokens=True, max_length=min(max_length, 512))\n",
    "input_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']\n",
    "```\n",
    "\n",
    "**Suggestion**: Hugginface's documentation is full of tutorials and user-friendly APIs.\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "**Question**: I'm hitting **out of memory error** when training my models, do you have any suggestions?\n",
    "\n",
    "**Answer**: Here are some common workarounds:\n",
    "\n",
    "1. Try decreasing the mini-batch size\n",
    "2. Try applying a different padding strategy (if you are applying padding): e.g. use quantiles instead of maximum sequence length\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c56a612",
   "metadata": {
    "id": "9c56a612"
   },
   "source": [
    "# Contact\n",
    "\n",
    "For any doubt, question, issue or help, you can always contact us at the following email addresses:\n",
    "\n",
    "Teaching Assistants:\n",
    "\n",
    "* Andrea Galassi -> a.galassi@unibo.it\n",
    "* Federico Ruggeri -> federico.ruggeri6@unibo.it\n",
    "\n",
    "Professor:\n",
    "\n",
    "* Paolo Torroni -> p.torroni@unibo.it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bac4b9",
   "metadata": {
    "id": "54bac4b9"
   },
   "source": [
    "# The End!\n",
    "\n",
    "Questions?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [
    "d26d68b7",
    "f6643e14"
   ],
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0359681dbee2483bab6a27ed3acff49a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "04c68dcd281545fe9647d533f2a2766d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0537d1f63b84469ba5e1d68ba28dc705": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "05b45d5259114a86972c561ee279d16b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_70008e8991924dc2b2b9600bd11e9520",
      "max": 1355863,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_554e6bca47bb4310af851b72c2d46185",
      "value": 1355863
     }
    },
    "05f9cdc9fd8f48819186ce3bf86bcc03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08977c2ff8b0476a81ac8482d99b684f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "09644168005c4c12aefcd12e9c145a46": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0dc50d96a9564ac3ba24e4d1d13050fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1044e634e0434d14986687a316bf53a0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "110b22e26654475c9945941995697401": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1e01f77e6f9942e781843d4d5677e6ec",
      "placeholder": "​",
      "style": "IPY_MODEL_48131b81011a4879a8bdb0e3a8d9e5e7",
      "value": " 232k/232k [00:00&lt;00:00, 501kB/s]"
     }
    },
    "156b0ba7aaa143c8bff2577f914da428": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a81829d0b28d456cbf2f3d79b415ecb2",
       "IPY_MODEL_76785112ba3042dfa848d646f46e2caf",
       "IPY_MODEL_a471a28c0f2844ba9e06d6e1688af6d9"
      ],
      "layout": "IPY_MODEL_277e13c0a47146849a5ea23f6699db06"
     }
    },
    "19df42f9eb2a41e387b5b2e8fcf0046b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c31073a23bc40e38fd8bd397815ca61": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1e01f77e6f9942e781843d4d5677e6ec": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "277e13c0a47146849a5ea23f6699db06": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2941cecad4744271ad885f98ebd5be02": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_08977c2ff8b0476a81ac8482d99b684f",
      "placeholder": "​",
      "style": "IPY_MODEL_fc0ef86fce88409fafc27019e4911d48",
      "value": "Downloading: 100%"
     }
    },
    "2cd2521bf6a940f1920b99c62d8e2b6c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3292384b6a2b4c62b48dc06ee7fcf5c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5638870ccb6444699937c56820961ec4",
      "max": 4,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ed8602b4418d4998b32935e4fcc46c1e",
      "value": 4
     }
    },
    "33b09adae2f24c73b7bff2aee5ab3b9d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "35ff2258a2c54aec83fee1b35bd67d86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e74c5807215848aca1c51107f18bfdc9",
      "placeholder": "​",
      "style": "IPY_MODEL_5174cb4266b44c09ac6031d43d6a754d",
      "value": " 1.36M/1.36M [00:00&lt;00:00, 4.61MB/s]"
     }
    },
    "3853181aea68478b8e320e379b49873b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a6664d49e15448a5b892596c952dbb1d",
      "placeholder": "​",
      "style": "IPY_MODEL_5abe54cb350941e7b44f1e344a64ecf2",
      "value": "Downloading: 100%"
     }
    },
    "47caf508f7b24c1ba30882ef41a8c68a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b402533437824be09721cf8f820585c1",
      "max": 898823,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_b479b1298372495c961e5a8c6a377e9e",
      "value": 898823
     }
    },
    "48131b81011a4879a8bdb0e3a8d9e5e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5174cb4266b44c09ac6031d43d6a754d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "52e821ce3416442bb1eb407b7553fd09": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7430f33e75d949528ac466d518c61979",
      "placeholder": "​",
      "style": "IPY_MODEL_67f7fe3ad85a4f0094bea0f3adef3f72",
      "value": " 456k/456k [00:00&lt;00:00, 611kB/s]"
     }
    },
    "554e6bca47bb4310af851b72c2d46185": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5638870ccb6444699937c56820961ec4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5a5a9e4d54b04c3c8f4017eb6a5aa356": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89a6215ea31c426ebfa53a76e505f5eb",
      "placeholder": "​",
      "style": "IPY_MODEL_cfc922b403504a0cba807023562d1874",
      "value": "Downloading: 100%"
     }
    },
    "5abe54cb350941e7b44f1e344a64ecf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5fa8cacb63f94b27a925f6d51a00c209": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_784ca36bb4b343eebc9049322420981b",
       "IPY_MODEL_83efa302cd8541e584a1a991fa1d331b",
       "IPY_MODEL_52e821ce3416442bb1eb407b7553fd09"
      ],
      "layout": "IPY_MODEL_09644168005c4c12aefcd12e9c145a46"
     }
    },
    "67f7fe3ad85a4f0094bea0f3adef3f72": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6b0519737d8c4491b69d3c6df4a5c519": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8909e8e5f2f54d8bb0cefd59ef802a3c",
       "IPY_MODEL_d7191c4685f94dd9acb935d5f6875118",
       "IPY_MODEL_7737112e6cde4de087c8f9e5a6ec5d61"
      ],
      "layout": "IPY_MODEL_1044e634e0434d14986687a316bf53a0"
     }
    },
    "6f77fc940c824cfe845929aa6cd179ad": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6f7baafc305c4eb99fd49d017794ddfc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "70008e8991924dc2b2b9600bd11e9520": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "71bc02dd3eb44115a5d7d75167acfe3b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_3853181aea68478b8e320e379b49873b",
       "IPY_MODEL_47caf508f7b24c1ba30882ef41a8c68a",
       "IPY_MODEL_8d501114e1354c14913cd12887eed765"
      ],
      "layout": "IPY_MODEL_19df42f9eb2a41e387b5b2e8fcf0046b"
     }
    },
    "720eb2b49bde42ff9668f0a2d83a0494": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7430f33e75d949528ac466d518c61979": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "76785112ba3042dfa848d646f46e2caf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_badccd1b8a244441a99ec98850dfe074",
      "max": 480,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_9c74667f30c0420894fe863d07fc32d9",
      "value": 480
     }
    },
    "7737112e6cde4de087c8f9e5a6ec5d61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca5fdbb8962649cd8b44aa6189f35a14",
      "placeholder": "​",
      "style": "IPY_MODEL_c4f3f36dea1b49c3a5d1fe29426e547e",
      "value": " 285/285 [00:00&lt;00:00, 6.28kB/s]"
     }
    },
    "784ca36bb4b343eebc9049322420981b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6f7baafc305c4eb99fd49d017794ddfc",
      "placeholder": "​",
      "style": "IPY_MODEL_8bc492f63dde4040a0d5b0093f28e4e6",
      "value": "Downloading: 100%"
     }
    },
    "7a31c757c383433390e2e973fc98c54d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a9921beff1a4c0490089672f06e2943": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2941cecad4744271ad885f98ebd5be02",
       "IPY_MODEL_05b45d5259114a86972c561ee279d16b",
       "IPY_MODEL_35ff2258a2c54aec83fee1b35bd67d86"
      ],
      "layout": "IPY_MODEL_0dc50d96a9564ac3ba24e4d1d13050fe"
     }
    },
    "7eac541155f8466c8ab758e871b1b0a5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "83efa302cd8541e584a1a991fa1d331b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_90e2155d9d294ecfbd5ed1cb4e5d7a6d",
      "max": 456318,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f3c48eff156e41fb97c39c732c32628f",
      "value": 456318
     }
    },
    "854de8f437cf4171b80b993c10bce051": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_05f9cdc9fd8f48819186ce3bf86bcc03",
      "placeholder": "​",
      "style": "IPY_MODEL_04c68dcd281545fe9647d533f2a2766d",
      "value": " 4/4 [00:06&lt;00:00,  1.62s/ba]"
     }
    },
    "866903075a5a4faba3eeb03cabb52ea1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5a5a9e4d54b04c3c8f4017eb6a5aa356",
       "IPY_MODEL_afc80424214445388b9557e9d44b4cef",
       "IPY_MODEL_110b22e26654475c9945941995697401"
      ],
      "layout": "IPY_MODEL_2cd2521bf6a940f1920b99c62d8e2b6c"
     }
    },
    "8909e8e5f2f54d8bb0cefd59ef802a3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8fb9e7e18a2c41a6a554679064be811a",
      "placeholder": "​",
      "style": "IPY_MODEL_0359681dbee2483bab6a27ed3acff49a",
      "value": "Downloading: 100%"
     }
    },
    "89a6215ea31c426ebfa53a76e505f5eb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8bc492f63dde4040a0d5b0093f28e4e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8d501114e1354c14913cd12887eed765": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a89a0cec2e8f48869af899b5b15c4625",
      "placeholder": "​",
      "style": "IPY_MODEL_33b09adae2f24c73b7bff2aee5ab3b9d",
      "value": " 899k/899k [00:00&lt;00:00, 737kB/s]"
     }
    },
    "8fb9e7e18a2c41a6a554679064be811a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90e2155d9d294ecfbd5ed1cb4e5d7a6d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "94adda85c3cf4f14ab615e0c280ca133": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9c74667f30c0420894fe863d07fc32d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "9ed6636d8224420e845c9e16bfed8f65": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a471a28c0f2844ba9e06d6e1688af6d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9ed6636d8224420e845c9e16bfed8f65",
      "placeholder": "​",
      "style": "IPY_MODEL_94adda85c3cf4f14ab615e0c280ca133",
      "value": " 480/480 [00:00&lt;00:00, 7.78kB/s]"
     }
    },
    "a6664d49e15448a5b892596c952dbb1d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a6f03eeb313c43f492829d66966e9abe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1c31073a23bc40e38fd8bd397815ca61",
      "placeholder": "​",
      "style": "IPY_MODEL_7a31c757c383433390e2e973fc98c54d",
      "value": "100%"
     }
    },
    "a81829d0b28d456cbf2f3d79b415ecb2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0537d1f63b84469ba5e1d68ba28dc705",
      "placeholder": "​",
      "style": "IPY_MODEL_ad7fedfefdc44a3e9af940c2b13da34c",
      "value": "Downloading: 100%"
     }
    },
    "a89a0cec2e8f48869af899b5b15c4625": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ad7fedfefdc44a3e9af940c2b13da34c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "af582ac6b439483bb4b28c6ad03c5f52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "afc80424214445388b9557e9d44b4cef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fd607a3be8eb4c199ad7745cce008eeb",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_af582ac6b439483bb4b28c6ad03c5f52",
      "value": 231508
     }
    },
    "b402533437824be09721cf8f820585c1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b479b1298372495c961e5a8c6a377e9e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "badccd1b8a244441a99ec98850dfe074": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c4f3f36dea1b49c3a5d1fe29426e547e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ca5fdbb8962649cd8b44aa6189f35a14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cfc922b403504a0cba807023562d1874": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d7191c4685f94dd9acb935d5f6875118": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_720eb2b49bde42ff9668f0a2d83a0494",
      "max": 285,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7eac541155f8466c8ab758e871b1b0a5",
      "value": 285
     }
    },
    "e478f51e1e8047e888c741b7b5a13244": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_a6f03eeb313c43f492829d66966e9abe",
       "IPY_MODEL_3292384b6a2b4c62b48dc06ee7fcf5c7",
       "IPY_MODEL_854de8f437cf4171b80b993c10bce051"
      ],
      "layout": "IPY_MODEL_6f77fc940c824cfe845929aa6cd179ad"
     }
    },
    "e74c5807215848aca1c51107f18bfdc9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ed8602b4418d4998b32935e4fcc46c1e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f3c48eff156e41fb97c39c732c32628f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "fc0ef86fce88409fafc27019e4911d48": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fd607a3be8eb4c199ad7745cce008eeb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
